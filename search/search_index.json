{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pisek \u23f3","text":"<p>Tool for developing tasks for programming competitions.</p>"},{"location":"#why-use-pisek","title":"Why use pisek?","text":"<ul> <li>Fast<ul> <li>Caching \u2014 Results from previous runs are cached and only the changes are tested.</li> <li>Parallelism \u2014 Pisek runs tests in parallel to speed up task testing.</li> </ul> </li> <li>Versatile<ul> <li>Many programming languages \u2014 Among them C, C++, Java, Bash, Python and Rust. Additionally, you can build your programs with a Makefile.</li> <li>Supports various program interfaces \u2014 We aim to support many interfaces used across different contests. If you have a program interface you would like us to support, please create an issue.</li> <li>Open-data and closed-data tasks \u2014 Pisek supports the preparation of both open-data tasks (where contestants get the input) and closed-data tasks (where contestants submit their code). </li> </ul> </li> <li>Customizable<ul> <li>You can set defaults for configurations across all tasks in your organization. </li> </ul> </li> </ul>"},{"location":"#who-is-using-pisek","title":"Who is using Pisek","text":"<p>Pisek is (was) used by:</p> <ul> <li> CEOI 2024</li> <li> Czech Informatics Olympiad</li> <li> Kasiopea</li> </ul> <p>If you used Pisek to prepare tasks for your competition, we would be more than happy to hear from you and add it to this list.</p>"},{"location":"CMS/","title":"CMS Integration","text":"<p>Pisek can automatically import tasks into the Contest Management System used at the International Olympiad in Informatics. Additionally, it can submit reference solutions as a test user and verify that all of them score as expected.</p>"},{"location":"CMS/#setup","title":"Setup","text":"<p>To import tasks, Pisek calls into CMS's internal libraries. Therefore, CMS and Pisek have to be installed in the same Python environment, be it both globally or in the same virtual environment. You can either install Pisek onto one of the servers hosting CMS, or you can install both onto a separate computer.</p> <p>Additionally, you will need to provide a <code>cms.toml</code> config file. Pisek uses CMS's utilities to locate it, so using it on a CMS server will require no further setup.</p>"},{"location":"CMS/#configuration","title":"Configuration","text":"<p>When importing into CMS, Pisek will use the same configuration as it uses during testing. However, there are some options are specific to importing into CMS. These reside in the <code>[cms]</code> section of the config file and are all optional. However, there are a couple of options that you'll probably want to change:</p> <pre><code>[cms]\nname=rabbit # The name of the task used in the URL and menu\ntitle=Rabbit pathfinding # The display name of the task\ntime_limit=1.5 # The time limit, in seconds\nmem_limit=512 # The memory limit, in mebibytes\n</code></pre> <p>For details on other options, see the example config.</p>"},{"location":"CMS/#importing-and-managing-tasks","title":"Importing and managing tasks","text":"<p>Tasks in CMS consist of two main parts: The task itself and its datasets. Contests don't always go as planned, which is why CMS allows changing testcases, output judges, time and memory limits and similar values on the fly. This is handled through datasets. When a change is needed, the organisers can create a new, hidden dataset with the given change, automatically re-evaluate all submissions in the background and finally atomically swap the datasets out. Some settings are stored in the task itself and cannot be changed using datasets. This includes the tasks name, the maximum number of submissions, or the number of decimal digits in scores.</p> <p>For more information on CMS tasks, see the documentation.</p> <p>Pisek allows you to manage both parts independently.</p>"},{"location":"CMS/#creating-a-new-task","title":"Creating a new task","text":"<p>Before anything else can be done, a task must be created in CMS. To do that, use the <code>create</code> subcommand of the <code>cms</code> module:</p> <pre><code>pisek cms create\n</code></pre> <p>This will first not only create a task with all the values specified in the task config, but it will also create the first dataset. It will also run and test the primary solution first, to generate inputs and outputs.</p> <p>The new task won't be part of any contest. To assign it to a contest, use the Admin Web Server.</p> <p>The description of the initial dataset can be changed using the <code>--description</code> or <code>-d</code> option:</p> <pre><code>pisek cms create -d \"My first dataset\"\n</code></pre>"},{"location":"CMS/#adding-a-dataset","title":"Adding a dataset","text":"<p>If you need to change testcases, judges, stubs, managers or resource limits before or during the contest, you need to create a new dataset. The <code>add</code> command allows you to just that:</p> <pre><code>pisek cms add\n</code></pre> <p>This will run the primary solution again, if needed, to generate new inputs and outputs. It will then upload them to the CMS and create a new dataset.</p> <p>Note that this will not update the task's properties. The only values from the <code>[cms]</code> section of the config file that have any effect are <code>time_limit</code> and <code>mem_limit</code>. To update the remaining options, use <code>pisek cms update</code>.</p> <p>This dataset won't be live, but it will be judged automatically in the background. If you don't want all submissions to be judged on this dataset automatically, you can use the <code>--no-autojudge</code> option:</p> <pre><code>pisek cms add --no-autojudge\"\n</code></pre> <p>To make a dataset live, use the Admin Web Server. The task page will contain a \"[Make live ...]\" button next above the dataset configuration.</p> <p>The description of the dataset can be specified using the <code>--description</code> or <code>-d</code> option:</p> <pre><code>pisek cms add -d \"Better dataset\"\n</code></pre> <p>By default, the description will be set to the current date and time.</p>"},{"location":"CMS/#updating-the-settings-of-a-task","title":"Updating the settings of a task","text":"<p>The <code>update</code> command allows you to update the task's settings:</p> <pre><code>pisek cms update\n</code></pre> <p>This will change the task's properties to match those defined in the <code>[cms]</code> section of the config file. Note that this will not create a new dataset. The <code>time_limit</code> and <code>mem_limit</code> options won't be updated, as those are stored inside datasets.</p>"},{"location":"CMS/#submitting-reference-solutions","title":"Submitting reference solutions","text":"<p>Since most task authors don't develop tasks on the machines used to host CMS's workers, it's important to try and evaluate reference solutions with CMS. Additionally, there are some differences between how CMS and Pisek executes programs.</p> <p>Before any submissions can be made, the tested task needs to be assigned to a contest. Since all submissions must be associated with a user, a testing user needs to be created and added to contest as well. It's recommended to mark the user's participation as hidden, as this will prevent it from showing up in the scoreboard.</p>"},{"location":"CMS/#submitting","title":"Submitting","text":"<p>To submit all reference solutions, simply use the <code>submit</code> command:</p> <pre><code>pisek cms submit -u [user]\n</code></pre> <p>Set the <code>-u</code>/<code>--username</code> argument to the username of the test user.</p> <p>It might take a while before the Evaluation Service notices the new submissions and starts evaluating them. To speed this process up, you can use the reevaluation buttons in the Admin Web Server, but make sure not to reevaluate more than what you want.</p> <p>The <code>submit</code> command checks which solutions have already been submitted, and won't submit them again.</p>"},{"location":"CMS/#checking-the-results","title":"Checking the results","text":"<p>Once the submissions have finished evaluating, you can check the results with the <code>check</code> command:</p> <pre><code>pisek cms check (-d DATASET | -a)\n</code></pre> <p>Set the <code>-d</code>/<code>--dataset</code> argument to the description of the dataset you're interested in. (Or <code>-a/--active-dataset</code> for the active dataset.)</p> <p>This will print out how many points each solution received, as well as how well it did on each subtask:</p> <pre><code>solve_fast: 100.0 points\n  Samples: 1.0\n  Subtask 1: 1.0\n  Subtask 2: 1.0\nsolve_slow: 60.0 points (should be 0)\n  Samples: 1.0\n  Subtask 1: 1.0 (should be wrong)\n  Subtask 2: 0.0\n</code></pre> <p>Any result that doesn't match the constraints defined in the config file will be highlighted in red.</p>"},{"location":"CMS/#generating-a-testing-log","title":"Generating a testing log","text":"<p>You can also generate a JSON file with details on how each solution did on each testcase. To do that, simply use the <code>testing-log</code> command:</p> <pre><code>pisek cms testing-log (-d DATASET | -a)\n</code></pre> <p>Again, set the <code>-d</code>/<code>--dataset</code> argument to the description of the target dataset. (Or <code>-a/--active-dataset</code> for the active dataset.)</p> <p>The format is compatible with the file generated when running Pisek with the <code>--testing-log</code> argument.</p>"},{"location":"cheatsheet/","title":"Cheatsheet","text":""},{"location":"cheatsheet/#task-creation","title":"Task creation","text":"<p>Create a task skeleton: <pre><code>pisek init\n</code></pre></p>"},{"location":"cheatsheet/#task-testing","title":"Task testing","text":"<p>Test the task: <pre><code>pisek test\n</code></pre></p> <p>Test only specific solutions: <pre><code>pisek test solutions solution1 solution2 ...\n</code></pre></p> <p>Test only generator: <pre><code>pisek test generator\n</code></pre></p>"},{"location":"cheatsheet/#testing-with-flags","title":"Testing with flags","text":"<p>Show file contents on failure: <pre><code>pisek test -C\n</code></pre></p> <p>Override time limit for solutions to 3 seconds: <pre><code>pisek test -t 3\n</code></pre></p> <p>Test all inputs (even when not necessary): <pre><code>pisek test -a\n</code></pre></p> <p>Be verbose: <pre><code>pisek test -v\n</code></pre></p> <p>Do final pre-production check. Test all inputs, be verbose and interpret warnings as failures: <pre><code>pisek test -a -v --strict\n</code></pre></p>"},{"location":"cheatsheet/#clean","title":"Clean","text":"<p>Clean pisek cache and created files (executables in <code>build/</code> and test data in <code>tests/</code>): <pre><code>pisek clean\n</code></pre></p>"},{"location":"cheatsheet/#visualization","title":"Visualization","text":"<p>Visualize all solutions and their closeness to time limit. Calculate valid time limits: <pre><code>pisek test -af --testing-log\npisek visualize | less -R\n</code></pre></p>"},{"location":"cheatsheet/#configs","title":"Configs","text":"<p>Update config to newest version: <pre><code>pisek config update\n</code></pre></p>"},{"location":"config-docs/","title":"Config documentation","text":"config-v3-documentation<pre><code># Example and explanation of a config file for a task made in pisek\n\n# Values beginning with ! or @ are reserved and have special meaning:\n# - Only ! value is !unset, which makes the key have no value (and thus ignore higher level defaults)\n# - @ values autoexpand depending on context\n\n# Bool type switches can have the following values:\n# True values:\n#   - 1, True, true, t, yes, y, on\n# False values:\n#   - 0, False, false, f, no, n, off\n\n\n[task]\n\nversion=v3\n# Task version\n# - v1 - Old version with a basic set of options. (default)\n# - v2 - Old version, with an expanded set of options.\n# - v3 - Current version, recommended\n# The version applies to this config file only and cannot be inherited with the use key.\n\n# You can use 'pisek config update' to update the config to the highest version.\n\nuse=organization-config\n# Config to use defaults from (defaults to none)\n\n# Values of keys are loaded in the following way:\n# 1. We try to find a given (section,key) in the task config\n# 2. If not found, we go to the config specified by the use key.\n# 3. If we are in the topmost config, we reset to the task config and\n#    try searching for the defaulting section and key of this key.\n# 4. If there is no defaulting key, we either return the default value, or fail if there is none.\n#\n# For example, consider:\n#   use=organization-config\n#   ([section],key) defaults to ([default_section],key)\n#   ([default_section],key) defaults to 42 if not present\n# Then we search in this order:\n# 1. this config,         [section], key\n# 2. organization-config, [section], key\n# 3. this config,         [default_section], key\n# 4. organization-config, [default_section], key\n# 5. If still not found, return 42\n\ntask_type=interactive\n# Task type:\n# - batch (default)\n# - interactive\n\nscore_precision=0\n# How many decimal digits scores are rounded to (defaults to 0)\n\n[tests]\n\nin_gen=gen\n# Reference to the run section specifying how to run the generator (if empty, only static inputs are used)\n# See [run] for more\n\ngen_type=opendata-v1\n# Specifies the generator type. (required for non-empty in_gen)\n# - pisek-v1 (recommended)\n# - cms-old\n# - opendata-v1\n# For more see docs/generator.md\n\nin_format=strict-text\nout_format=text\n# Format of input/output:\n#   text        UTF-8 or UTF-16 encoded ASCII printable characters, with an optional BOM. (default for out_format)\n#               All lines (including the last one) are automatically converted to be terminated with a LF character.\n#   strict-text ASCII printable characters. (default for in_format)\n#               All lines (including the last one) must already be terminated with a LF character.\n#   binary      Can be anything.\n#\n# If the input does not conform to in_format, failure is immediately reported.\n# If the output does not conform to out_format, it gets the 'normalization fail' verdict.\n\nvalidator=validate\n# Reference to the run section specifying how to run the validator\n# See [run] for more\n#\n# No value means no checking (default)\n\nvalidator_type=simple-42\n# Specifies the validator type (required for non-empty validator)\n# - simple-42 (recommended)\n# - simple-0\n# For more see docs/validator.md\n\nout_check=judge\n# Describes how to check outputs (required)\n#   - diff: compare with correct output (discouraged, can be slow in some cases)\n#   - tokens: compare token-by-token with correct output (tokens are separated by whitespace)\n#   - shuffle: like tokens, but allow permutation of tokens\n#   - judge: check with a custom program (called a 'judge')\n# In interactive, only judge is allowed\n# For more see docs/batch_checker.md or docs/interactive_judge.md\n\nout_judge=judge\n# Only for out_check=judge (required in that case)\n# Reference to the run section specifying how to run the judge\n# See [run] for more\n\njudge_type=cms-batch\n# Only for out_check=judge (required in that case)\n# Specifies how to call judge and how judge reports result\n#\n# For task_type=batch:\n# - cms-batch\n#       https://cms.readthedocs.io/en/v1.4/Task%20types.html?highlight=manager#checker\n# - opendata-v1\n#       ./judge &lt;test&gt; &lt;seed&gt; &lt; output\n#       $TEST_INPUT=[input] $TEST_OUTPUT=[correct_output]\n#       return code 0 means correct, 1 wrong\n#\n# For task_type=interactive:\n# - cms-communication\n#       https://cms.readthedocs.io/en/v1.4/Task%20types.html?highlight=manager#communication\n#\n# See docs/batch_checker.md or docs/interactive_judge.md for details\n\njudge_needs_in=0\njudge_needs_out=0\n# Only for task_type=batch and out_check=judge\n# Whether the judge needs the input/correct output (bool)\n# Both default to true\n\ntokens_ignore_newlines=0\n# Only for out_check=tokens\n# If set to true, newline characters will be ignored when checking the output,\n# as if they were any other whitespace characters\n# If set to false, newline characters are only ignored at the end of the file (default)\n\ntokens_ignore_case=0\n# Only for out_check=tokens\n# If set to true, ASCII characters will be compared in a case-insensitive manner (defaults to false)\n\ntokens_float_rel_error=0.00001\ntokens_float_abs_error=1e-30\n# Only for out_check=tokens\n# When these options are specified, floating-point numbers\n# will be parsed and compared with a given error margin\n# Any tokens that can't be parsed as a float will be compared character-by-character\n# If used, both of these options must be specified\n# To explicitly disable float checking, set both options to the empty string (default)\n\nshuffle_mode=lines\n# Only for out_check=shuffle (required in that case)\n# Which permutations are allowed:\n#   lines       Permutation of lines\n#   words       Permutation of words within each line\n#   lines_words Both lines and words\n#   tokens      Permutation of all tokens, ignoring line boundaries\n\nshuffle_ignore_case=0\n# Only for out_check=shuffle\n# If set to true, ASCII characters will be compared in a case-insensitive manner (defaults to false)\n\nstatic_subdir=static_tests/\n# Try to find static inputs and outputs in this folder relative to config (defaults to .)\n\n# Defaults for [testXX] keys:\n\nname=@auto\n# Name of the test\n# @auto expands to \"Test [test number]\" (default)\n\npoints=2\n# Number of points for this test (non-negative int | \"unscored\")\n#\n# If points=unscored, it is not possible to get any points for this test.\n# (Unlike points=0, where some judge_types might give more than the maximum number of points.)\n#\n# (required in [tests] or each [testXX])\n\nin_globs=@ith\n# Which new inputs are introduced in this test.\n# Supports expansion of * and ? as in shell\n# @ith expands to {test_number:02}*.in (default)\n\npredecessors=\n# Space separated list of test easier than this test\n# Inputs from these test are included into this test as well\n# @previous expands to previous test (or nothing if test has number &lt;= 1)\n\n\n# Keys for enabling/disabling per-test checks that the task must satisfy\n# See also the [checks] section for whole-task checks\n#\n# Please be careful when disabling checks as it can transform\n# task preparation into a minefield\n#\n# Each value is a bool:\n\nchecks.validate=on\n# If on, validate this test (defaults to on)\n\nchecks.different_outputs=on\n# Checks that not all of the primary solution's outputs on this test are the same (defaults to on)\n# (only if there are at least 2 testcases)\n\n[test01]\n# Section for each test (indexed from one)\n# Keys default to [tests] keys if not set\nname=Test 1\npoints=3\nin_globs=01*.in\npredecessors=\nchecks.validate=on\nchecks.different_outputs=true\n\n[test00]\n# Section for samples test\n# Can be omitted and has the following defaults:\nname=Samples\npoints=unscored\nin_globs=sample*.in\npredecessors=\n\n[solution_correct]\n# Section for each solution\n# This solution is named \"correct\"\n# Keys default to [solution] keys\n\nrun=solve\n# Reference to the run section specifying how to run this solution\n# See [run] for more\n#\n# @auto expands to name of the section without \"solution_\" (in this case to \"correct\") (default)\n\nprimary=yes\n# Use this solution to generate correct outputs? (bool, defaults to false)\n# Exactly one solution has to be set to primary\n# (or zero if there are no solutions in config)\n\npoints=10\n# Points that program should get or X for any number of points (defaults to X)\n\npoints_min=5\npoints_max=7\n# Upper and lower bounds on points (defaults to X)\n# Cannot be set simultaneously with points\n\ntests=X10\n# String describing result on each test:\n# 1 - success\n# 0 - fail\n# P - partial success\n# S - superoptimal success\n# A - success / superoptimal success\n# W - wrong answer\n# ! - runtime error\n# T - timeout\n# N - normalization fail\n# X - any result\n#\n# The result of a test is the result of the worst testcase.\n# W!TN require at least one testcase with the corresponding verdict.\n#\n# @all - string of 11...\n# @any - string of XX...\n# @auto - @all if this is primary solution, @any otherwise (default)\n\n[solutions]\n# Defaults for all solutions\n# If not specified the defaults are:\nrun=@auto\nprimary=no\npoints=X\npoints_min=X\npoints_max=X\ntests=@auto\n\n# There are also keys that are specific to [solutions]\n# and cannot be configured on a per solution basis:\n\n[run]\n# Sections describing running a program.\n#\n# The run section for each program is optional. If it is missing, its contents are autogenerated,\n# by looking for most specific section with the given key set, according to this hierarchy:\n#                                          [run]\n#       [run_gen]          [run_validator]          [run_judge]                         [run_solution]\n#                                                                    [run_primary_solution] [run_secondary_solution]\n# [run_gen:{program}] [run_validator:{program}] [run_judge:{program}]              [run_solution:{program}]\n\nbuild=@auto\n# Reference to the build section specifying how to build this program\n# @auto expands to '{program_role}:{subdir}/{program}', e.g. 'solution:solutions/solve' (default)\n# See [build] for more\n\nexec=@auto\n# Filename of the program to execute relative to the built directory\n# If a directory is chosen directory/run is executed instead\n# Defaults to empty (which chooses the built file / directory itself)\n\nargs=--slow\n# Additional arguments for the program. (Given before any other arguments.)\n# (defaults to empty)\nsubdir=\n# Subdirectory from where to look for the program (relative to the task directory)\n# (defaults to empty)\n\n# Execution limits, setting limit to 0 means unlimited. Defaults are the following:\ntime_limit=360\n# Execution time limit [seconds] (defaults to 360 s)\nclock_mul=\n# Wall clock multiplier [1] (defaults to 2x)\nclock_min=\n# Wall clock minimum [second] (defaults to 1 s)\nmem_limit=\n# Memory limit [MB] (defaults to unlimited)\nprocess_limit=\n# Maximum number of processes -- at the moment,\n# limits greater than 1 are interpreted as \"unlimited\". (defaults to 1)\n# Please keep in mind that killing multiple processes\n# upon errors is inherently unreliable.\n\nenv_KEY=VALUE\n# Sets an environment variable when running this program.\n# For example this sets the environment variable KEY to have value VALUE\n\n[run_gen]\n# Overriding process_limit for generator\nprocess_limit=0\n\n[run_solution]\n# Overriding time_limit for solutions\ntime_limit=3\n\n[run_solution:slow]\n# Overriding time_limit for specific solution\ntime_limit=10\n\n[build]\n# Sections describing building a program.\n#\n# The build section for each program is optional. If it is missing, its contents are autogenerated,\n# by looking for most specific section with the given key set, according to this hierarchy:\n#                                                    [build]\n#                          [build_gen]           [build_validator]           [build_judge]           [build_solution]\n#   [build:{program}] [build_gen:{program}] [build_validator:{program}] [build_judge:{program}] [build_solution:{program}]\n#\n# Build sections must have different {program} suffixes. For multi-role programs, using [build:{program}] is recommended.\n\nsources=@auto\n# List of sources needed for building the executable\n# @auto expands to {program} (default)\ncomp_args=\n# Additional compiler arguments (Given after any other arguments.)\n# Defaults to empty\nextras=\n# Additional files to be copied to the build directory\nentrypoint=\n# For building some executables (e.g. python), an entrypoint is needed.\n# Defaults to empty\nstrategy=auto\n# Build strategy for building this program\n# 'auto' detects automatically which strategy to use (default)\n# Other strategies are:\n# - python\n# - shell\n# - c\n# - cpp\n# - pascal\n# - java\n# - make\n# - cargo\n\nheaders_c=\n# Headers to include in the c strategy (defaults to empty)\nextra_sources_c=\n# Extra source files for the c strategy (defaults to empty)\nheaders_cpp=\n# Headers to include in the cpp strategy (defaults to empty)\nextra_sources_cpp=\n# Extra source files for the cpp strategy (defaults to empty)\nextra_sources_java=\n# Extra source files for the java strategy (defaults to empty)\nextra_sources_py=\n# Extra source files for the python strategy (defaults to empty)\n\n# It is recommended to set these keys in higher sections ([build_solution], [build],...)\n# and use the sources and extra keys for program-specific sections\n\n[limits]\n# Section with input/output size limits.\n\ninput_max_size=20\n# Maximal input size [MB]\n# (0 for unlimited) (defaults to 50)\n\noutput_max_size=5\n# Maximal input size [MB]\n# (0 for unlimited) (defaults to 10)\n\n[checks]\n# Section for enabling/disabling whole-task checks that the task must satisfy\n# See also the checks keys in [tests] for per-test checks\n#\n# Please be careful when disabling checks as it can transform\n# task preparation into a minefield\n#\n# Each value is a bool:\n\nsolution_for_each_test=off\n# Checks that a dedicated solution exists for each test (aside from samples) (defaults to off)\n# A dedicated solution for a test is one that:\n# - Gets full points on this test and its predecessors.\n# - Doesn't get full points on other test.\n\nno_unused_inputs=on\n# Checks that there are no unused inputs in the entire task: (defaults to on)\n# - In static_subdir\n# - Generated by generator\n\nall_inputs_in_last_test=off\n# Checks that all inputs are included in the last test (defaults to off)\n\ngenerator_respects_seed=on\n# Checks that the generator generates two different inputs for two different seeds (defaults to on)\n\none_input_in_each_nonsample_test=off\n# Checks that each test (excluding samples) contains exactly one input (defaults to off)\n# Useful for opendata tasks\n\nfuzzing_thoroughness=250\n# Checks that the judge doesn't crash on randomly generated malicious outputs.\n# They are generated by modifying correct outputs:\n# - 1/5 are cut in the middle\n# - 4/5 by substituting a token for another one\n\n# Number of malicious outputs to test. (defaults to 250) Set to 0 to disable.\n\n\n[cms]\n# Settings related to the CMS importer\n# See CMS docs (https://cms.readthedocs.io/en/latest/) for details\n\nname=a-plus-b\n# Name of task, which will appear in the task URL (required for CMS commands)\ntitle=A plus B\n# The name of the task shown on the task description page\n# @name expands to the task name (default)\nsubmission_format=adder.%l\n# The name of the submitted file\n# .%l will be replaced with the language's file extension\n# @name expands to to the task name with non-alphanumeric characters replaced with _ and .%l appended (default)\n\ntime_limit=1\n# Execution time limit [seconds] (defaults to 1)\nmem_limit=1024\n# Memory limit [MB] (defaults to 1024)\n\nmax_submissions=50\n# The number of submissions one contestant allowed to make, or X for unlimited (defaults to 50)\nmin_submission_interval=60\n# The number of seconds a contestant has to wait between consecutive submissions (defaults to 0)\n\nscore_mode=max_subtask\n# Describes how the final score is computed from the scores of individual submissions\n# May be 'max', 'max_subtask' (default) or 'max_tokened_last'\nfeedback_level=restricted\n# Specifies how much information is given to the contestants about their submission\n# May be 'full', 'restricted' (default) or 'oi_restricted'\n\nstubs=src/stub\n# Only for C/C++/Python\n# Stubs to upload to CMS (without suffix)\n# @auto expands to the union of [build_solution] extra_sources_* keys (default)\n# Used commonly for interactive tasks\n\nheaders=src/a-plus-b.h\n# Only for C/C++\n# Headers to upload to CMS\n# @auto expands to the union of [build_solution] headers_* keys (default)\n# Used commonly for interactive tasks\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>You can install Pisek simply by running: <pre><code>pip install pisek\n</code></pre></p> <p>For upgrading, add <code>--upgrade</code>: <pre><code>pip install pisek --upgrade\n</code></pre></p> <p>Or if you can't wait for the latest features, you can download pisek directly from GitHub : <pre><code>pip install git+https://github.com/piskoviste/pisek\n</code></pre></p>"},{"location":"quickstart/","title":"Usage","text":""},{"location":"quickstart/#creating-a-task","title":"Creating a task","text":"<p>First, you will need a task to test. You can create a task skeleton by running:</p> <pre><code>pisek init\n</code></pre> <p>You can also choose one of our example tasks to try pisek on.</p>"},{"location":"quickstart/#what-a-task-looks-like","title":"What a task looks like","text":"<p>A task is a single directory containing programs, task data, and most importantly a <code>config</code> file which holds all the metadata for the task (scoring, limits, how to run programs, etc.).</p> <p>Programs have their roles (generator, solution, judge, \u2026) specified in the <code>config</code>. However, it is customary to also give them self-descriptive names such as <code>gen.py</code> or <code>solve_slow.cpp</code>.</p> <p>Additionally, static inputs (<code>*.in</code>) and outputs (<code>*.out</code>) are contained in the top-level folder or in <code>static_subdir</code>, as specified in the <code>config</code>.</p>"},{"location":"quickstart/#testing-tasks","title":"Testing tasks","text":"<p>The task is tested by running: <pre><code>pisek test\n</code></pre></p> <p>For testing only some solutions, you can use: <pre><code>pisek test solutions solution1 solution2 ...\n</code></pre></p> <p>Similarly, for testing just the generator: <pre><code>pisek test generator\n</code></pre></p>"},{"location":"quickstart/#cleaning","title":"Cleaning","text":"<p>During task testing, pisek generates binaries, testing data, caches, logs, etc. You can remove all these files with: <pre><code>pisek clean\n</code></pre></p>"},{"location":"quickstart/#visualization","title":"Visualization","text":"<p>Finally, you can visualize solution times and their closeness to the time limit: <pre><code>pisek test --testing-log\npisek visualize\n</code></pre></p>"},{"location":"task-overview/","title":"Task overview","text":"<p>The Task overview is meant for those who are familiar with competitive programming but haven't made any tasks yet. If you have already set tasks before, you can just look at the task type diagrams for a batch task and an interactive task to brush up on potential differences in terminology.</p>"},{"location":"task-overview/#task-parts","title":"Task parts","text":"<p>There are several task parts the author needs to write:</p>"},{"location":"task-overview/#task-statement","title":"Task statement","text":"<p>The task statement is there for contestant to read. Pisek doesn't handle task statements, so you are free to write them however you want.</p>"},{"location":"task-overview/#tests","title":"Tests","text":"<p>Pisek however does need to know about tests (groups of testcases). Those usually are some additional restrictions on the task statements to allow weaker solutions to get some points. These can be lower limits for slower solutions, or special cases of the original task statement.</p> <p>Some tasks have only one test (i.e. ICPC tasks) while others have many (IOI tasks). Tests need to be entered in the config.</p>"},{"location":"task-overview/#solution","title":"Solution","text":"<p>The first and best known task component is the solution. It is the same as what the contestant should write - taking the input and producing the output.</p> <p>One of the solutions should be the primary solution, always producing the correct output. It is also recommended to write some wrong solutions to ensure they don't pass.</p>"},{"location":"task-overview/#generator","title":"Generator","text":"<p>The generator is used for generating inputs that the solution is tested on. Ideally, the generator should generate diverse enough inputs to break any wrong solution.</p>"},{"location":"task-overview/#checker","title":"Checker","text":"<p>The checker is used for determining whether a given solution is correct. It greatly differs between task types, so you can read more there.</p> <p>A task-specific checker provided by the task author is called a judge.</p>"},{"location":"task-overview/#validator","title":"Validator","text":"<p>The validator is used for making sure that inputs produced by the generator conform to the task statement. It adds an additional degree of safety.</p>"},{"location":"task-overview/#task-types","title":"Task types","text":"<p>There are a few types of tasks pisek supports:</p> <ol> <li>Batch tasks</li> <li>Interactive tasks</li> </ol>"},{"location":"task-overview/#batch-task","title":"Batch task","text":"<pre><code>graph TD;\n    G[Generator] --&gt;|Input| V[Validator];\n    G --&gt;|Input| S[Solution];\n    G --&gt;|Input| CS[Correct solution];\n    S --&gt;|Output| C[Checker];\n    CS --&gt;|Correct output| C;</code></pre>"},{"location":"task-overview/#batch-checker","title":"Batch checker","text":"<p>A batch checker gets the solution output and determines whether it is correct. It can also get the correct output (from the primary solution) if specified in the config.</p>"},{"location":"task-overview/#interactive-task","title":"Interactive task","text":"<pre><code>graph TD;\n    G[Generator] --&gt;|Input| C[Validator];\n    G --&gt;|Input| J[Judge];\n    J ==&gt;|Communication| S[Solution];\n    S ==&gt;|Communication| J;</code></pre> <p>An interactive task is used when a part of the input should remain hidden from the contestant's solution. For example, when it can ask questions about the input.</p>"},{"location":"task-overview/#interactive-judge","title":"Interactive judge","text":"<p>The judge in an interactive task gets the task input and is run together with the solution. The solution can make requests to the judge about the input. Finally, the judge determines whether the solution is correct.</p> <p>One example would be that the judge gets a hidden sequence in the input. The solution then makes queries to the judge and reconstructs the hidden sequence. After giving this sequence to the judge, the solution is marked by the judge as correct on this input.</p>"},{"location":"task-parts/batch-checker/","title":"Batch checker","text":"<p>The batch checker gets the contestant's output, the input, and the correct output. It should determine whether the contestant's output is correct.</p> <p>There are various types of checkers you can use:</p> <ul> <li>tokens \u2013 fast, versatile file equality checker</li> <li>shuffle \u2013 similar to tokens, but allows permutations of tokens</li> <li>diff \u2013 file equality checker based on the <code>diff</code> command line tool (avoid this option, as it has quadratic time complexity)</li> <li>judge \u2013 custom checker</li> </ul> <p>If there is only a single correct output (e.g. the minimum of an array), <code>tokens</code> is strongly recommended. Otherwise, when there are multiple correct outputs (e.g. the shortest path in a graph), writing a judge is necessary. Set <code>out_check</code> in the config accordingly.</p>"},{"location":"task-parts/batch-checker/#tokens-checker","title":"Tokens checker","text":"<p>A fast and versatile equality checker. Ignores whitespace, but not newlines. (Ignores newlines only at the end of a file.)</p> <p>Tokens are separated by (possibly multiple) whitespace characters. For the output to be correct, the tokens need to be same as in the correct output file.</p> <p>You can customize the tokens checker with <code>tokens_ignore_newlines</code> or <code>tokens_ignore_case</code>. For comparing floats, set <code>tokens_float_rel_error</code> and <code>tokens_float_abs_error</code>. Details can be found in config-documentation.</p>"},{"location":"task-parts/batch-checker/#shuffle-checker","title":"Shuffle checker","text":"<p>Similarly to the tokens checker, the shuffle checker compares the output with the correct output token-by-token. Allows permutations of tokens (permutations can be configured with <code>shuffle_mode</code>). Use <code>shuffle_ignore_case</code> for case insensitivity.</p>"},{"location":"task-parts/batch-checker/#diff-checker","title":"Diff checker","text":"<p>An equality checker based on the <code>diff</code> tool. Runs <code>diff -Bbq</code> under the hood. Ignores whitespace and empty lines.</p> This <code>out_check</code> is not recommended <p>In some cases, <code>diff</code> has quadratic time complexity, leading to unexpectedly slow checking of outputs.</p>"},{"location":"task-parts/batch-checker/#custom-judge","title":"Custom judge","text":"<p>If there can be multiple correct solutions, it is necessary to write a custom judge. Set <code>out_judge</code> to the path to the source code of your judge, <code>judge_type</code> to the judge type (see below), and <code>judge_needs_in</code>, <code>judge_needs_out</code> to <code>0</code>/<code>1</code>, depending on whether the judge needs the input and the correct output.</p> <p>When writing a custom judge, you can choose from multiple judge types:</p> <ol> <li>cms-batch judge</li> <li>opendata-v2</li> <li>opendata-v1</li> </ol>"},{"location":"task-parts/batch-checker/#cms-batch-judge","title":"CMS-batch judge","text":"<p>The CMS batch judge format as described in the CMS documentation.</p> <p>It is run as follows (having filenames given as arguments): <pre><code>./judge &lt;input&gt; &lt;correct output&gt; &lt;contestant output&gt;\n</code></pre></p> <p>The judge should print a relative number of points (a float between 0.0 and 1.0) to its stdout as a single line. To its stderr it should write a single-line message to the contestant. Unlike what the CMS documentation specifies, the files should be single-line only. There will be a warning otherwise.</p> Example <code>cms-batch</code> judge <p>For a task of printing N positive integers that sum up to K, the judge may look like this: <pre><code>#!/usr/bin/env python3\nimport sys\nfrom typing import NoReturn\n\n\ndef award(points: float, msg: str) -&gt; NoReturn:\n    print(points)\n    print(msg, file=sys.stderr)\n    exit(0)\n\n\ndef reject() -&gt; NoReturn:\n    award(0, \"translate:wrong\")\n\n\ninput_, correct_output, contestant_output = sys.argv[1:]\n\nwith open(input_) as f:\n    n, k = map(int, f.readline().split())\n\nwith open(contestant_output) as f:\n    try:\n        numbers = list(map(int, f.readline().split()))\n    except ValueError:\n        reject()  # The contestant did not print integers\n    except EOFError:\n        reject()  # The contestant output ends\n\n    if f.read().strip():\n        reject()  # Contestant output doesn't end when it should\n\n\n# Be careful to check **ALL** constraints\nif any(map(lambda x: x &lt;= 0, numbers)):\n    reject()\n\nif sum(numbers) == k:\n    award(1.0, \"translate:success\")\nelif sum(numbers) &gt;= k / 2:\n    award(0.5, \"translate:partial\")\nelse:\n    reject()  # Not enough sand used\n</code></pre></p>"},{"location":"task-parts/batch-checker/#opendata-v2-judge","title":"Opendata-v2 judge","text":"<p>The opendata-v2 judge is run in this way: <pre><code>./judge &lt;test&gt; &lt;seed&gt; &lt; contestant-output\n</code></pre> Where <code>test</code> is the testcase's test number and <code>seed</code> the testcase's generating seed. (The arguments are the same as those given to the <code>opendata-v1</code> generator this input has (probably) been generated with.) If the input was not generated with a seed (static or unseeded), <code>seed</code> will be <code>-</code>.</p> <p>If <code>judge_needs_in</code> is set, the judge will get the input filename in the <code>TEST_INPUT</code> environment variable. Similarly, if <code>judge_needs_out</code> is set, the correct output filename will be in the <code>TEST_OUTPUT</code> environment variable.</p> <p>If the output is correct, the judge should exit with return code 42. Otherwise, the judge should exit return code 43.</p> <p>Optionally, the judge can write a one-line message for the contestant to stderr (at most 255 bytes), followed by a sequence of lines with <code>KEY=value</code> pairs. The following keys are allowed:</p> <ul> <li><code>POINTS</code> \u2013 Number of points awarded for this test case (used only if the exit code says \"OK\").</li> <li><code>LOG</code> \u2013 A message that should be logged.</li> <li><code>NOTE</code> \u2013 An internal note recorded in the database, but not visible to contestants.</li> </ul> <p>Values are again limited to 255 bytes.</p> Example <code>opendata-v2</code> judge <p>For a task of printing N positive integers that sum up to K, the judge may look like this: <pre><code>#!/usr/bin/env python3\nimport os\nimport sys\nfrom typing import NoReturn\n\nSUBTASK_POINTS = [0, 20, 30, 50]\n\n\ndef award(points: float | None, msg: str) -&gt; NoReturn:\n    assert \"\\n\" not in msg\n    print(msg, file=sys.stderr)\n    print(f\"POINTS={points}\", file=sys.stderr)\n    exit(42)\n\n\ndef reject(msg: str) -&gt; NoReturn:\n    assert \"\\n\" not in msg\n    print(msg, file=sys.stderr)\n    exit(43)\n\n\ndef main(subtask: int, seed: str) -&gt; NoReturn:\n    # Read the test input\n    with open(os.environ[\"TEST_INPUT\"]) as f:\n        n, k = map(int, f.readline().split())\n\n    # Read the contestant output\n    try:\n        numbers = list(map(int, input().split()))\n    except ValueError:\n        reject(\"The output should contain integers.\")\n    except EOFError:\n        reject(\"The output is empty.\")\n\n    try:\n        input()\n        reject(\"The output should contain only one line.\")\n    except EOFError:\n        pass\n\n    # Be careful to check **ALL** constraints\n    if any(map(lambda x: x &lt;= 0, numbers)):\n        reject(\"The output contains negative integers.\")\n\n    if sum(numbers) == k:\n        award(SUBTASK_POINTS[subtask], \"All of the sand used.\")\n    elif sum(numbers) &gt;= k / 2:\n        award(SUBTASK_POINTS[subtask] // 2, \"At least half of the sand used.\")\n    else:\n        reject(\"Not enough sand used.\")\n\n\nif __name__ == \"__main__\":\n    main(subtask=int(sys.argv[1]), seed=sys.argv[2])\n</code></pre></p>"},{"location":"task-parts/batch-checker/#opendata-v1-judge","title":"Opendata-v1 judge","text":"<p>The opendata-v1 judge is the same as opendata-v2, with the exception of using different return codes, return code 0 for a correct output and return code 1 for a wrong output.</p> This <code>judge_type</code> is not recommended <p>Return with exit code 1 is very common and is for example trigger by any exception in Python. This can lead to internal judge bugs disguising themselves as wrong answers.</p>"},{"location":"task-parts/generator/","title":"Generator","text":"<p>The generator is used for generating inputs. Solutions are then run and judged on those.</p>"},{"location":"task-parts/generator/#generator-type","title":"Generator type","text":"<p>There are currently 3 generator types available:</p> <ul> <li>pisek-v1</li> <li>cms-old</li> <li>opendata-v1</li> </ul> <p>However, we strongly recommend using the first (pisek-gen) type for better debugging and easy conversion between open data and closed data tasks.</p>"},{"location":"task-parts/generator/#terminology","title":"Terminology","text":"<p>There are two requirements for generators:</p> <ul> <li>Generators must be deterministic. For the same arguments, it should always generate the same input(s).</li> <li>If a generator takes a seed as an argument, the generator should respect the seed. For different seeds,   it should generate different inputs. This can be disabled in the <code>[checks]</code> section, but be careful.</li> </ul>"},{"location":"task-parts/generator/#pisek-v1","title":"Pisek-v1","text":""},{"location":"task-parts/generator/#listing-inputs","title":"Listing inputs","text":"<p>When run without arguments it should list all inputs it can generate in the following format: <pre><code>input_name key1=value1 key2=value2\n</code></pre> Where <code>input_name</code> is the name of the given input. The input will be generated into the file <code>[input_name]_[seed].in</code> or <code>[input_name]</code> (depending whether the input is seeded). This is followed by any number of key=value pairs separated by spaces. The following keys are supported:</p> Key Meaning Value type Default value repeat How many times should this input be generated? int 1 seeded Is this input generated with a random seed? bool true <p>If the input is not seeded, repeat must be 1.</p> <p>For example: <pre><code>01_tree\n02_random_graph repeat=10\n02_complete_graph seeded=false\n</code></pre></p> Example <code>pisek-v1</code> generator <p>For a task of printing N positive integers that sum up to K, the generator may look like this: <pre><code>#!/usr/bin/env python3\nimport itertools\nimport random\nimport sys\n\nFEATURES = [\"few\", \"small\", \"equal\", \"max\"]\n\n\ndef gen(input_name: str) -&gt; None:\n    max_n = 10 if \"few\" in input_name else 10**5\n    max_k = 10**5 if \"small\" in input_name else 10**9\n\n    if \"max\" in input_name:\n        n = max_n\n        k = max_k\n    else:\n        n = random.randint(1, max_n)\n        k = random.randint(n, max_k)\n\n    if \"equal\" in input_name:\n        k = n\n\n    print(n, k)\n\n\n# No arguments - List all inputs we can generate\nif len(sys.argv) == 1:\n    for variant in itertools.product([True, False], repeat=4):\n        features = [f for i, f in enumerate(FEATURES) if variant[i]]\n        if \"equal\" in features and \"small\" not in features:\n            continue  # Any equal input is small\n\n        if features:\n            print(\"_\".join(features), end=\" \")\n            if \"max\" in features:\n                # max is deterministic - no need for seed\n                print(\"seeded=false\")\n            else:\n                # Otherwise generate this input three times\n                print(\"repeat=3\")\n        else:\n            print(\"big repeat=5\")\n\n# Generate an input\nelse:\n    # Input is seeded\n    if len(sys.argv) == 3:\n        random.seed(sys.argv[2])\n\n    gen(sys.argv[1])\n</code></pre></p>"},{"location":"task-parts/generator/#generating-inputs","title":"Generating inputs","text":"<p>The generator is then repeatedly asked to generate the input <code>input_name</code> from the inputs list.</p> <p>If <code>input_name</code> is seeded, the generator is run with: <pre><code>./gen &lt;input_name&gt; &lt;seed&gt;\n</code></pre> Where <code>seed</code> is a 16-digit hexadecimal number. The generator must be deterministic and respect the seed.</p> <p>If <code>input_name</code> is unseeded, the generator is called with <pre><code>./gen &lt;input_name&gt;\n</code></pre> The generator must be deterministic.</p> <p>In either case, the generator should print the input to its stdout.</p>"},{"location":"task-parts/generator/#cms-old","title":"Cms-old","text":"<p>The generator is run with: <pre><code>./gen &lt;directory&gt;\n</code></pre></p> <p>The generator should generate all input files to this directory. The generator must be deterministic.</p> This <code>gen_type</code> is not recommended <p>We don't recommend using this generator type for these reasons:</p> <ul> <li>It's hard to debug, as you don't know during the generation of what input the bug occurs.</li> <li>You need to write your own management code for which inputs to generate, how many times, and what seeds to use.</li> </ul>"},{"location":"task-parts/generator/#opendata-v1","title":"Opendata-v1","text":"<p>The generator is run: <pre><code>./gen &lt;test&gt; &lt;seed&gt;\n</code></pre> Where <code>seed</code> is 16-digit hexadecimal number.</p> <p>The generator should generate the input for this test to its stdout. The generator must be deterministic and respect the given seed.</p> <p>(Please note that the generator can generate only one input for each test.)</p> Example <code>opendata-v1</code> generator <p>For a task of printing N positive integers that sum up to K, the generator may look like this: <pre><code>#!/usr/bin/env python3\nimport itertools\nimport random\nimport sys\n\n\ndef gen(test: int) -&gt; None:\n    max_n = 10**5\n    max_k = 10**5 if test == 2 else 10**9\n\n    # Dividing max_n by four actually makes stronger tests\n    # as sandcastles of size 1 doesn't use half of the sand\n    n = random.randint(1, max_n // 4)\n    k = random.randint(n, max_k)\n\n    if test == 1:\n        k = n\n\n    print(n, k)\n\n\nrandom.seed(sys.argv[2])\ngen(int(sys.argv[1]))\n</code></pre></p>"},{"location":"task-parts/interactive-judge/","title":"Interactive judge","text":"<p>The interactive judge gets the input and the means of communication with the contestant's solution. It should report whether the contestant's solution is correct.</p> <p>There is currently only one interactive <code>judge_type</code> supported:</p>"},{"location":"task-parts/interactive-judge/#cms-communication-judge","title":"CMS-communication judge","text":"<p>See CMS documentation for more. However, only one user process is supported.</p> <p>The solution and the judge run simultaneously. They communicate via FIFO (named pipes).</p> <p>The judge is run: <pre><code>./judge &lt;FIFO from solution&gt; &lt;FIFO to solution&gt; &lt; input\n</code></pre> FIFOs point to the solution's stdout and stdin respectively.</p> <p>The judge should print a relative number of points (a float between 0.0 and 1.0) to its stdout as a single line. To its stderr it should write a single-line message to the contestant. Unlike what the CMS documentation specifies, the files should be single-line only. There will be a warning otherwise.</p>"},{"location":"task-parts/validator/","title":"Validator","text":"<p>The validator is used for validating inputs, i.e. making sure they conform to the restrictions in the task statement.</p>"},{"location":"task-parts/validator/#validator-type","title":"Validator type","text":"<p>There are currently 2 validator types available:</p> <ul> <li>simple-42</li> <li>simple-0</li> </ul>"},{"location":"task-parts/validator/#simple-42","title":"Simple-42","text":"<p>The simple-42 validator is run as follows: <pre><code>./validate &lt;test_num&gt; &lt; input\n</code></pre> Where <code>test_num</code> is the number of the test to which the input belongs. If the input is valid, the validator should exit with return code 42. Otherwise, it should exit with any other return code.</p> Example <code>simple-42</code> validator <p>For a task of printing N positive integers that sum up to K, the validator may look like this: <pre><code>#!/usr/bin/env python3\nimport sys\n\ntest = int(sys.argv[1])\nn, k = map(int, input().split(\" \"))  # We use explicitly split(\" \") to be more strict\n\nassert 1 &lt;= n &lt;= 10**5, f\"{n=} limits\"\nassert 1 &lt;= k &lt;= 10**9, f\"{k=} limits\"\n\nif test == 1:\n    assert n == k, \"N and K should be equal\"\nelif test == 2:\n    assert k &lt;= 10**5, f\"{k=} is too big\"\n\ntry:\n    input()\nexcept EOFError:\n    exit(42)  # The input is valid\n\nassert False, \"The input doesn't end when it should\"\n</code></pre></p>"},{"location":"task-parts/validator/#simple-0","title":"Simple-0","text":"<p>The simple-0 validator is run as follows: <pre><code>./validate &lt;test_num&gt; &lt; input\n</code></pre> Where <code>test_num</code> is the number of the test to which the input belongs. If the input is valid, the validator should exit with return code 0. Otherwise, it should exit with any other return code.</p> This <code>validator_type</code> is not recommended <p>Return with exit code 0 can be sometimes caused by libraries. It is better to avoid this pitfall and use the simple-42 validator type instead.</p>"}]}