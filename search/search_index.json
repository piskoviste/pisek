{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pisek \u23f3","text":"<p>Tool for developing tasks for programming competitions.</p>"},{"location":"#why-use-pisek","title":"Why use pisek?","text":"<ul> <li>Fast<ul> <li>Caching \u2014 Results from previous runs are cached and only the changes are tested.</li> <li>Parallelism \u2014 Pisek runs tests in parallel to speed up task testing.</li> </ul> </li> <li>Versatile<ul> <li>Many programming languages \u2014 Among them C, C++, Java, Bash, Python and Rust. Additionally, you can build your programs with a Makefile.</li> <li>Supports various program interfaces \u2014 We aim to support many interfaces used across different contests. If you have a program interface you would like us to support, please create an issue.</li> <li>Open-data and closed-data tasks \u2014 Pisek supports the preparation of both open-data tasks (where contestants get the input) and closed-data tasks (where contestants submit their code). </li> </ul> </li> <li>Customizable<ul> <li>You can set defaults for configurations across all tasks in your organization. </li> </ul> </li> </ul>"},{"location":"#who-is-using-pisek","title":"Who is using Pisek","text":"<p>Pisek is (was) used by:</p> <ul> <li> CEOI 2024</li> <li> Czech Informatics Olympiad</li> <li> Kasiopea</li> </ul> <p>If you used Pisek to prepare tasks for your competition, we would be more than happy to hear from you and add it to this list.</p>"},{"location":"CMS/","title":"CMS Integration","text":"<p>Pisek can automatically import tasks into the Contest Management System used at the International Olympiad in Informatics. Additionally, it can submit reference solutions as a test user and verify that all of them score as expected.</p>"},{"location":"CMS/#setup","title":"Setup","text":"<p>To import tasks, Pisek calls into CMS's internal libraries. Therefore, CMS and Pisek have to be installed in the same Python environment, be it both globally or in the same virtual environment. You can either install Pisek onto one of the servers hosting CMS, or you can install both onto a separate computer.</p> <p>Additionally, you will need to provide a <code>cms.toml</code> config file. Pisek uses CMS's utilities to locate it, so using it on a CMS server will require no further setup.</p>"},{"location":"CMS/#configuration","title":"Configuration","text":"<p>When importing into CMS, Pisek will use the same configuration as it uses during testing. However, there are some options are specific to importing into CMS. These reside in the <code>[cms]</code> section of the config file and are all optional. However, there are a couple of options that you'll probably want to change:</p> <pre><code>[cms]\nname=rabbit # The name of the task used in the URL and menu\ntitle=Rabbit pathfinding # The display name of the task\ntime_limit=1.5 # The time limit, in seconds\nmem_limit=512 # The memory limit, in mebibytes\n</code></pre> <p>For details on other options, see the example config.</p>"},{"location":"CMS/#importing-and-managing-tasks","title":"Importing and managing tasks","text":"<p>Tasks in CMS consist of two main parts: The task itself and its datasets. Contests don't always go as planned, which is why CMS allows changing testcases, output judges, time and memory limits and similar values on the fly. This is handled through datasets. When a change is needed, the organisers can create a new, hidden dataset with the given change, automatically re-evaluate all submissions in the background and finally atomically swap the datasets out. Some settings are stored in the task itself and cannot be changed using datasets. This includes the tasks name, the maximum number of submissions, or the number of decimal digits in scores.</p> <p>For more information on CMS tasks, see the documentation.</p> <p>Pisek allows you to manage both parts independently.</p>"},{"location":"CMS/#creating-a-new-task","title":"Creating a new task","text":"<p>Before anything else can be done, a task must be created in CMS. To do that, use the <code>create</code> subcommand of the <code>cms</code> module:</p> <pre><code>pisek cms create\n</code></pre> <p>This will first not only create a task with all the values specified in the task config, but it will also create the first dataset. It will also run and test the primary solution first, to generate inputs and outputs.</p> <p>The new task won't be part of any contest. To assign it to a contest, use the Admin Web Server.</p> <p>The description of the initial dataset can be changed using the <code>--description</code> or <code>-d</code> option:</p> <pre><code>pisek cms create -d \"My first dataset\"\n</code></pre>"},{"location":"CMS/#adding-a-dataset","title":"Adding a dataset","text":"<p>If you need to change testcases, judges, stubs, managers or resource limits before or during the contest, you need to create a new dataset. The <code>add</code> command allows you to just that:</p> <pre><code>pisek cms add\n</code></pre> <p>This will run the primary solution again, if needed, to generate new inputs and outputs. It will then upload them to the CMS and create a new dataset.</p> <p>Note that this will not update the task's properties. The only values from the <code>[cms]</code> section of the config file that have any effect are <code>time_limit</code> and <code>mem_limit</code>. To update the remaining options, use <code>pisek cms update</code>.</p> <p>This dataset won't be live, but it will be judged automatically in the background. If you don't want all submissions to be judged on this dataset automatically, you can use the <code>--no-autojudge</code> option:</p> <pre><code>pisek cms add --no-autojudge\"\n</code></pre> <p>To make a dataset live, use the Admin Web Server. The task page will contain a \"[Make live ...]\" button next above the dataset configuration.</p> <p>The description of the dataset can be specified using the <code>--description</code> or <code>-d</code> option:</p> <pre><code>pisek cms add -d \"Better dataset\"\n</code></pre> <p>By default, the description will be set to the current date and time.</p>"},{"location":"CMS/#updating-the-settings-of-a-task","title":"Updating the settings of a task","text":"<p>The <code>update</code> command allows you to update the task's settings:</p> <pre><code>pisek cms update\n</code></pre> <p>This will change the task's properties to match those defined in the <code>[cms]</code> section of the config file. Note that this will not create a new dataset. The <code>time_limit</code> and <code>mem_limit</code> options won't be updated, as those are stored inside datasets.</p>"},{"location":"CMS/#submitting-reference-solutions","title":"Submitting reference solutions","text":"<p>Since most task authors don't develop tasks on the machines used to host CMS's workers, it's important to try and evaluate reference solutions with CMS. Additionally, there are some differences between how CMS and Pisek executes programs.</p> <p>Before any submissions can be made, the tested task needs to be assigned to a contest. Since all submissions must be associated with a user, a testing user needs to be created and added to contest as well. It's recommended to mark the user's participation as hidden, as this will prevent it from showing up in the scoreboard.</p>"},{"location":"CMS/#submitting","title":"Submitting","text":"<p>To submit all reference solutions, simply use the <code>submit</code> command:</p> <pre><code>pisek cms submit -u [user]\n</code></pre> <p>Set the <code>-u</code>/<code>--username</code> argument to the username of the test user.</p> <p>It might take a while before the Evaluation Service notices the new submissions and starts evaluating them. To speed this process up, you can use the reevaluation buttons in the Admin Web Server, but make sure not to reevaluate more than what you want.</p> <p>The <code>submit</code> command checks which solutions have already been submitted, and won't submit them again.</p>"},{"location":"CMS/#checking-the-results","title":"Checking the results","text":"<p>Once the submissions have finished evaluating, you can check the results with the <code>check</code> command:</p> <pre><code>pisek cms check (-d DATASET | -a)\n</code></pre> <p>Set the <code>-d</code>/<code>--dataset</code> argument to the description of the dataset you're interested in. (Or <code>-a/--active-dataset</code> for the active dataset.)</p> <p>This will print out how many points each solution received, as well as how well it did on each subtask:</p> <pre><code>solve_fast: 100.0 points\n  Samples: 1.0\n  Subtask 1: 1.0\n  Subtask 2: 1.0\nsolve_slow: 60.0 points (should be 0)\n  Samples: 1.0\n  Subtask 1: 1.0 (should be wrong)\n  Subtask 2: 0.0\n</code></pre> <p>Any result that doesn't match the constraints defined in the config file will be highlighted in red.</p>"},{"location":"CMS/#generating-a-testing-log","title":"Generating a testing log","text":"<p>You can also generate a JSON file with details on how each solution did on each testcase. To do that, simply use the <code>testing-log</code> command:</p> <pre><code>pisek cms testing-log (-d DATASET | -a)\n</code></pre> <p>Again, set the <code>-d</code>/<code>--dataset</code> argument to the description of the target dataset. (Or <code>-a/--active-dataset</code> for the active dataset.)</p> <p>The format is compatible with the file generated when running Pisek with the <code>--testing-log</code> argument.</p>"},{"location":"cheatsheet/","title":"Cheatsheet","text":""},{"location":"cheatsheet/#task-creation","title":"Task creation","text":"<p>Create a task skeleton: <pre><code>pisek init\n</code></pre></p>"},{"location":"cheatsheet/#task-testing","title":"Task testing","text":"<p>Test the task: <pre><code>pisek test\n</code></pre></p> <p>Test only specific solutions: <pre><code>pisek test solutions solution1 solution2 ...\n</code></pre></p> <p>Test only generator: <pre><code>pisek test generator\n</code></pre></p>"},{"location":"cheatsheet/#testing-with-flags","title":"Testing with flags","text":"<p>Show file contents on failure: <pre><code>pisek test -C\n</code></pre></p> <p>Override time limit for solutions to 3 seconds: <pre><code>pisek test -t 3\n</code></pre></p> <p>Test all inputs (even when not necessary): <pre><code>pisek test -a\n</code></pre></p> <p>Be verbose: <pre><code>pisek test -v\n</code></pre></p> <p>Do final pre-production check. Test all inputs, be verbose and interpret warnings as failures: <pre><code>pisek test -a -v --strict\n</code></pre></p>"},{"location":"cheatsheet/#clean","title":"Clean","text":"<p>Clean pisek cache and created files (executables in <code>build/</code> and test data in <code>tests/</code>): <pre><code>pisek clean\n</code></pre></p>"},{"location":"cheatsheet/#visualization","title":"Visualization","text":"<p>Visualize all solutions and their closeness to time limit. Calculate valid time limits: <pre><code>pisek test -af --testing-log\npisek visualize | less -R\n</code></pre></p>"},{"location":"cheatsheet/#configs","title":"Configs","text":"<p>Export task config to make it independent of the organization one: <pre><code>pisek config export\n</code></pre></p>"},{"location":"config-v3-documentation/","title":"Config v3 documentation","text":"<p>If you are just exploring, this page might be a bit daunting. Don't worry though, you usually need just a small subset of the keys. Check out our example tasks for how an usual config might look like.</p>"},{"location":"config-v3-documentation/#key-metadata","title":"Key metadata","text":"<p>Our keys use the following metadata tags:</p>"},{"location":"config-v3-documentation/#metadata-version","title":"\u2013 Version","text":"<p>The last version the key was greatly changed. This is either its creation, a key rename or a change of the default value. (Although we try to be mostly backwards compatible, the <code>version</code> <code>v3</code> of the config format was experimental for a long time.)</p>"},{"location":"config-v3-documentation/#type","title":"\u2013 Type","text":"<p>Type of the value. See value types.</p>"},{"location":"config-v3-documentation/#default","title":"\u2013 Default","text":"<p>Default value for this key.</p>"},{"location":"config-v3-documentation/#empty-default","title":"\u2013 Empty default","text":"<p>The default value is empty.</p>"},{"location":"config-v3-documentation/#required","title":"\u2013 Required","text":"<p>You must set this key.</p>"},{"location":"config-v3-documentation/#required-if-applicable","title":"\u2013 Required if applicable","text":"<p>Depending on other key values, you must either set this key or you cannot use this key.</p>"},{"location":"config-v3-documentation/#experimental","title":"\u2013 Experimental","text":"<p>This key is still new and can change.</p>"},{"location":"config-v3-documentation/#value-types","title":"Value types","text":"<p>Config keys are internally converted to various types.</p> <p>These can be basic:</p> <ul> <li>string</li> <li>int</li> <li>decimal \u2014 Both <code>3.14</code> and <code>2e5</code> forms are supported.</li> <li>bool<ul> <li>True values: <code>1</code>, <code>True</code>, <code>true</code>, <code>t</code>, <code>yes</code>, <code>y</code>, <code>on</code></li> <li>False values: <code>0</code>, <code>False</code>, <code>false</code>, <code>f</code>, <code>no</code>, <code>n</code>, <code>off</code></li> </ul> </li> <li>enum \u2014 One of the values specified in the key description.</li> <li>'string literal'</li> <li>run_ref \u2014 String referencing a <code>[run]</code> section. Must match the regex <code>[A-Za-z0-9][A-Za-z0-9./_^+-]*</code>.</li> <li>build_ref \u2014 String referencing a <code>[build]</code> section. Must match the regex <code>[A-Za-z0-9][A-Za-z0-9./_^+-]*</code>.</li> </ul> <p>Info</p> <p>Through auto-expansion and auto-generation run_ref and build_ref values can interpreted as paths.</p> <p>But if you are not writing your <code>[run]</code> or <code>[build]</code> section (and that's not usually needed), just treat these types as the path to your program.</p> <p>Filesystem:</p> <ul> <li>path \u2014 String, but additionally corresponds to a path in your filesystem.<ul> <li>Usually relative to the task directory.</li> <li>Usually already existing.</li> <li>Usually without the file extension.</li> </ul> </li> <li>glob \u2014 A glob describing a set of paths supporting expansion of <code>*</code> and <code>?</code> as in a shell.</li> </ul> <p>And generics:</p> <ul> <li>T | U \u2014 Either type T or type U</li> <li>list[T] \u2014 Space separated list of values of type T.</li> </ul>"},{"location":"config-v3-documentation/#reserved-values","title":"Reserved values","text":"<p>Values beginning with <code>!</code> or <code>@</code> are reserved and have special meaning:</p> <ul> <li>The only <code>!</code> value is <code>!unset</code>, which makes the key have no value (and thus ignore higher level defaults).</li> <li><code>@</code> values autoexpand depending on context.</li> </ul>"},{"location":"config-v3-documentation/#task","title":"[task]","text":""},{"location":"config-v3-documentation/#version","title":"version","text":"<p>1.0.0 enum v1</p> <p>Version of the task configuration file:</p> <ul> <li><code>v1</code> - Old version, with a basic set of options.</li> <li><code>v2</code> - Old version, with an expanded set of options.</li> <li><code>v3</code> - Current version, recommended</li> </ul> <p>The version applies to this config file only and cannot be inherited with the use key. You can use <code>pisek config update</code> to update the config to the highest version.</p>"},{"location":"config-v3-documentation/#use","title":"use","text":"<p>1.1.0 path </p> <p>Config to use defaults from.</p> <p>Values of keys are loaded in the following way:</p> <ol> <li>We try to find a given (section, key) in the task config.</li> <li>If not found, we go to the config specified by the use key.</li> <li>If the use key in the current config is empty, we go to pisek's global defaults.</li> <li>If we can't find the (section, key) there, we return to step 1 to    try searching for the defaulting section and key of this key.</li> <li>If there is no defaulting key, we fail as this key is required.</li> </ol> <p>For example, consider:</p> <ul> <li><code>use=organization-config</code></li> <li>(<code>[section]</code>,<code>key</code>) defaults to (<code>[default_section]</code>,<code>key</code>)</li> <li>(<code>[default_section]</code>,<code>key</code>) defaults to <code>42</code> if not present</li> </ul> <p>Then we search in this order:</p> config section key this config <code>[section]</code> <code>key</code> organization-config <code>[section]</code> <code>key</code> pisek's global-defaults <code>[section]</code> <code>key</code> this config <code>[default_section]</code> <code>key</code> organization-config <code>[default_section]</code> <code>key</code> pisek's global-defaults <code>[default_section]</code> <code>key</code>"},{"location":"config-v3-documentation/#task_type","title":"task_type","text":"<p>1.0.0 enum batch</p> <p>Task type:</p> <ul> <li><code>batch</code></li> <li><code>interactive</code></li> </ul>"},{"location":"config-v3-documentation/#score_precision","title":"score_precision","text":"<p>1.4.0 int 0</p> <p>How many decimal digits scores are rounded to.</p>"},{"location":"config-v3-documentation/#tests","title":"[tests]","text":"<p>Section for configuring the tests. </p> <p>Some keys in it serve as defaults for <code>[testXY]</code>. (More details there.)</p>"},{"location":"config-v3-documentation/#has_sample_test","title":"has_sample_test","text":"<p>2.3.0 bool yes</p> <p>Whether this task has <code>[test00]</code>.</p>"},{"location":"config-v3-documentation/#in_gen","title":"in_gen","text":"<p>1.0.0 run_ref </p> <p>Reference to the run section specifying how to run the generator. (If empty, only static inputs are used.)</p> <p>See <code>[run]</code> for more.</p>"},{"location":"config-v3-documentation/#gen_type","title":"gen_type","text":"<p>1.3.0 enum </p> <p>Specifies the generator type. (Required for non-empty <code>in_gen</code>.)</p> <ul> <li><code>pisek-v1</code> (recommended)</li> <li><code>cms-old</code></li> <li><code>opendata-v1</code></li> </ul> <p>For more info see generator.</p>"},{"location":"config-v3-documentation/#in_format","title":"in_format","text":"<p>1.7.0 enum strict-text</p>"},{"location":"config-v3-documentation/#out_format","title":"out_format","text":"<p>1.7.0 enum text</p> <p>Format of input/output:</p> <ul> <li><code>text</code> \u2014 UTF-8 or UTF-16 encoded ASCII printable characters (+ space, tab, LF, CR), with an optional BOM.   The file will be converted to plain, 8-bit ASCII before being passed to another program.   All lines (including the last one) are additionally automatically converted to be terminated with a LF character.</li> <li><code>strict-text</code> \u2014 ASCII printable characters (+ space, tab, LF, CR). All lines (including the last one)   must already be terminated with a LF character.</li> <li><code>binary</code> \u2014 Can be anything.</li> </ul> <p>If the input does not conform to <code>in_format</code>, failure is immediately reported. If the output does not conform to <code>out_format</code>, it gets the normalization fail verdict.</p>"},{"location":"config-v3-documentation/#validator","title":"validator","text":"<p>1.6.0 run_ref </p> <p>Reference to the run section specifying how to run the validator. See <code>[run]</code> for more.</p> <p>No value means no checking.</p>"},{"location":"config-v3-documentation/#validator_type","title":"validator_type","text":"<p>2.0.0 enum </p> <p>Specifies the validator type (required for non-empty <code>validator</code>)</p> <ul> <li><code>simple-42</code> (recommended)</li> <li><code>simple-0</code></li> </ul> <p>For more info see validator.</p>"},{"location":"config-v3-documentation/#out_check","title":"out_check","text":"<p>1.0.0 enum </p> <p>Describes how to check outputs.</p> <ul> <li><code>diff</code> \u2014 Compare with correct output. (Discouraged option, can be slow in some cases.)</li> <li><code>tokens</code> \u2014 Compare token-by-token with correct output (tokens are separated by whitespace).   Newlines are interpreted as tokens unless disabled. (Newlines at the end of the file are ignored.)</li> <li><code>shuffle</code> \u2014 Like <code>tokens</code>, but allow permutation of tokens.</li> <li><code>judge</code> \u2014 Check with a custom program (called a 'judge').</li> </ul> <p>For <code>task_type=interactive</code>, only <code>judge</code> is allowed.</p> <p>For more info see batch checker or interactive judge.</p>"},{"location":"config-v3-documentation/#out_judge","title":"out_judge","text":"<p>1.0.0 run_ref </p> <p>Only for <code>out_check=judge</code> (required in that case).</p> <p>Reference to the run section specifying how to run the judge. See <code>[run]</code> for more.</p>"},{"location":"config-v3-documentation/#judge_type","title":"judge_type","text":"<p>1.1.0 enum </p> <p>Only for <code>out_check=judge</code> (required in that case).</p> <p>Specifies how to call the judge and how the judge reports the verdict.</p> <p>For <code>task_type=batch</code>:</p> <ul> <li><code>cms-batch</code></li> <li><code>opendata-v1</code></li> </ul> <p>For <code>task_type=interactive</code>:</p> <ul> <li><code>cms-communication</code></li> </ul> <p>See batch checker or interactive judge for details.</p>"},{"location":"config-v3-documentation/#judge_needs_in","title":"judge_needs_in","text":"<p>1.0.0 bool true</p> <p>Only for <code>out_check=judge</code>.</p> <p>Whether the judge needs the input for checking.</p>"},{"location":"config-v3-documentation/#judge_needs_out","title":"judge_needs_out","text":"<p>1.0.0 bool true</p> <p>Only for <code>out_check=judge</code>.</p> <p>Whether the judge needs the correct output for checking.</p>"},{"location":"config-v3-documentation/#tokens_ignore_newlines","title":"tokens_ignore_newlines","text":"<p>1.2.0 bool false</p> <p>Only for <code>out_check=tokens</code>.</p> <p>If set to <code>true</code>, newline characters will be ignored when checking the output, as if they were any other whitespace characters. Otherwise, newline characters are only ignored at the end of the file.</p>"},{"location":"config-v3-documentation/#tokens_ignore_case","title":"tokens_ignore_case","text":"<p>1.2.0 bool false</p> <p>Only for <code>out_check=tokens</code>.</p> <p>If set to <code>true</code>, ASCII characters will be compared in a case-insensitive manner.</p>"},{"location":"config-v3-documentation/#tokens_float_rel_error","title":"tokens_float_rel_error","text":"<p>1.2.0 decimal </p>"},{"location":"config-v3-documentation/#tokens_float_abs_error","title":"tokens_float_abs_error","text":"<p>1.2.0 decimal </p> <p>Only for <code>out_check=tokens</code>.</p> <p>When these options are specified, floating-point numbers will be parsed and compared with a given error margin. Any tokens that can't be parsed as a float will be compared character-by-character. If used, both of these options must be specified. To explicitly disable float checking, set both options to the empty string.</p>"},{"location":"config-v3-documentation/#shuffle_mode","title":"shuffle_mode","text":"<p>1.5.0 enum </p> <p>Only for <code>out_check=shuffle</code> (required in that case).</p> <p>Which permutations are allowed:</p> <ul> <li><code>lines</code> \u2014 Permutation of lines</li> <li><code>words</code> \u2014 Permutation of words within each line</li> <li><code>lines_words</code> \u2014 Both lines and words</li> <li><code>tokens</code> \u2014 Permutation of all tokens, ignoring line boundaries</li> </ul>"},{"location":"config-v3-documentation/#shuffle_ignore_case","title":"shuffle_ignore_case","text":"<p>1.5.0 bool false</p> <p>Only for <code>out_check=shuffle</code>.</p> <p>If set to <code>true</code>, ASCII characters will be compared in a case-insensitive manner.</p>"},{"location":"config-v3-documentation/#static_subdir","title":"static_subdir","text":"<p>1.0.0 path .</p> <p>Try to find static inputs and outputs in this folder.</p>"},{"location":"config-v3-documentation/#name","title":"name","text":"<p>1.0.0 string @auto</p> <p>Name of this test. Default for <code>name</code> in each <code>[testXY]</code>.</p> <p><code>@auto</code> expands to <code>Test {test number}</code>.</p>"},{"location":"config-v3-documentation/#points","title":"points","text":"<p>1.0.0 decimal | 'unscored' </p> <p>Number of points for this test. Default for <code>points</code> in each <code>[testXY]</code>.</p> <p>If <code>points=unscored</code>, it is not possible to get any points for this test. (Unlike <code>points=0</code>, where some <code>judge_type</code>s might give more than the maximum number of points.)</p> <p>(Required in <code>[tests]</code> or each <code>[testXY]</code>.)</p>"},{"location":"config-v3-documentation/#in_globs","title":"in_globs","text":"<p>1.0.0 list[glob] @ith</p> <p>Which inputs this test contains. Gets combined with inputs specified by the <code>predecessors</code> key. Default for <code>in_globs</code> in each <code>[testXY]</code>.</p> <p>The globs select the input filenames they match. The filenames for unseeded inputs are in the form <code>{name}.in</code> and for seeded inputs <code>{name}_{seed}.in</code>. The globs must match the <code>_{seed}</code> part with <code>*</code> and end with <code>.in</code>.</p> <p><code>@ith</code> expands to <code>{test_number:02}*.in</code>.</p>"},{"location":"config-v3-documentation/#predecessors","title":"predecessors","text":"<p>1.0.0 list[int] </p> <p>List of tests easier than this test. Default for <code>predecessors</code> in each <code>[testXY]</code>.</p> <p>Inputs from these tests are included into this test as well.</p> <p><code>@previous</code> expands to previous test (or nothing if the test has number \u2264 1).</p> <p>Keys for enabling/disabling checks that the task must satisfy.</p> <p>Danger</p> <p>Please be careful when disabling checks as it can transform task preparation into a minefield.</p>"},{"location":"config-v3-documentation/#checksno_unused_inputs","title":"checks.no_unused_inputs","text":"<p>2.3.0 bool on</p> <p>Checks that there are no unused inputs in the entire task, both in the <code>static_subdir</code> or generated by the generator.</p>"},{"location":"config-v3-documentation/#checksall_inputs_in_last_test","title":"checks.all_inputs_in_last_test","text":"<p>2.3.0 bool off</p> <p>Checks that all inputs are included in the last test.</p>"},{"location":"config-v3-documentation/#checksone_input_in_each_nonsample_test","title":"checks.one_input_in_each_nonsample_test","text":"<p>2.3.0 bool off</p> <p>Checks that each test (excluding samples) contains exactly one input. Useful for opendata tasks.</p>"},{"location":"config-v3-documentation/#checksgenerator_respects_seed","title":"checks.generator_respects_seed","text":"<p>2.3.0 bool on</p> <p>Checks that the generator generates two different inputs for two different seeds.</p>"},{"location":"config-v3-documentation/#checksvalidate","title":"checks.validate","text":"<p>2.0.0 bool on</p> <p>If <code>on</code>, run the <code>validator</code> on inputs from this test.</p>"},{"location":"config-v3-documentation/#checkssolution_for_this_test","title":"checks.solution_for_this_test","text":"<p>2.3.0 bool off</p> <p>Checks that a dedicated solution exists for this test (ignored for <code>[test00]</code>).</p> <p>A dedicated solution for a test is one that:</p> <ul> <li>Gets full points on this test and its predecessors.</li> <li>Doesn't get full points on all other tests.</li> </ul>"},{"location":"config-v3-documentation/#checksdifferent_outputs","title":"checks.different_outputs","text":"<p>2.0.0 bool on</p> <p>Checks that not all of the primary solution's outputs on this test are the same. (Only if there are at least 2 testcases.)</p>"},{"location":"config-v3-documentation/#checksfuzzing_thoroughness","title":"checks.fuzzing_thoroughness","text":"<p>2.3.0 int 250</p> <p>Only for <code>out_check=judge</code>.</p> <p>Checks that the judge doesn't crash on randomly generated malicious outputs. They are generated by modifying correct outputs:</p> <ul> <li>1/10 are cut off in the middle.</li> <li>1/10 have a line replaced by an empty line</li> <li>4/5 have a token substituted for another one.</li> </ul> <p>The key specifies the number of malicious outputs to test. Set to <code>0</code> to disable.</p>"},{"location":"config-v3-documentation/#checksjudge_rejects_trailing_string","title":"checks.judge_rejects_trailing_string","text":"<p>2.3.0 bool on</p> <p>Only for <code>out_check=judge</code>.</p> <p>Checks that outputs that have a trailing string added to the end are judged as wrong answer.</p>"},{"location":"config-v3-documentation/#opendataonline_validity","title":"opendata.online_validity","text":"<p>2.3.0 int | 'unlimited' 300</p> <p>Reserved key for the opendata module.</p>"},{"location":"config-v3-documentation/#testxy","title":"[testXY]","text":"<p>A section for a specific test.</p> <p>The test sections are numbered <code>[test00]</code>, <code>[test01]</code>, <code>[test02]</code>,\u2026 The <code>[test00]</code> section is optional, while all others (up to the number of tests) are required.</p> <p>The allowed keys in this section are:</p> <ul> <li><code>name</code></li> <li><code>points</code></li> <li><code>in_globs</code></li> <li><code>predecessors</code></li> <li><code>checks.generator_respects_seed</code></li> <li><code>checks.validate</code></li> <li><code>checks.solution_for_this_test</code></li> <li><code>checks.different_outputs</code></li> </ul> <p>If they are not provided, their default from the <code>[tests]</code> section is used.</p> <p>Task with two tests</p> <ul> <li>Easy test contains inputs <code>01*.in</code></li> <li>Hard test contains inputs <code>01*.in</code> and <code>02*.in</code></li> </ul> <pre><code>[test01]\nname=Easy test\npoints=4\n\n[test02]\nname=Hard test\npoints=6\npredecessors=1\n</code></pre>"},{"location":"config-v3-documentation/#test00","title":"[test00]","text":"<p>The section for the test containing samples. Has different defaults for a few keys:</p> <pre><code>name=Samples\npoints=unscored\nin_globs=sample*.in\nopendata.online_validity=unlimited\n</code></pre>"},{"location":"config-v3-documentation/#solutions","title":"[solutions]","text":"<p>Defaults for all solutions.</p>"},{"location":"config-v3-documentation/#solution-run","title":"run","text":"<p>1.6.0 run_ref @auto</p> <p>Reference to the run section specifying how to run this solution. See <code>[run]</code> for more</p> <p><code>@auto</code> expands to name of the section without <code>solution_</code>. (For example, if the section name were <code>[solution_correct]</code>, <code>@auto</code> would expand to <code>correct</code>.)</p>"},{"location":"config-v3-documentation/#primary","title":"primary","text":"<p>1.0.0 bool false</p> <p>Use this solution to generate the reference outputs?</p> <p>Exactly one solution has to be set to primary (or zero if there are no solutions in the config).</p>"},{"location":"config-v3-documentation/#tests_1","title":"tests","text":"<p>1.6.0 string @auto</p> <p>A string describing the expected result on each test. It should have one character for each test, including <code>[test00]</code>.</p> <p>Recognized results are:</p> <ul> <li><code>1</code> \u2014 success</li> <li><code>0</code> \u2014 fail</li> <li><code>P</code> \u2014 partial success</li> <li><code>S</code> \u2014 superoptimal success</li> <li><code>A</code> \u2014 success / superoptimal success</li> <li><code>W</code> \u2014 wrong answer</li> <li><code>!</code> \u2014 runtime error</li> <li><code>T</code> \u2014 timeout</li> <li><code>N</code> \u2014 normalization fail</li> <li><code>X</code> \u2014 any result</li> </ul> <p>The result of a test is the result of the worst testcase. <code>W!TN</code> require at least one testcase with the corresponding verdict.</p> <ul> <li><code>@all</code> \u2014 string of <code>11...</code></li> <li><code>@any</code> \u2014 string of <code>XX...</code></li> <li><code>@auto</code> \u2014 <code>@all</code> if this is primary solution, <code>@any</code> otherwise</li> </ul>"},{"location":"config-v3-documentation/#points_1","title":"points","text":"<p>1.0.0 decimal | 'X' X</p> <p>Points that program should get or <code>X</code> for any number of points.</p>"},{"location":"config-v3-documentation/#points_min","title":"points_min","text":"<p>1.7.0 decimal | 'X' X</p>"},{"location":"config-v3-documentation/#points_max","title":"points_max","text":"<p>1.7.0 decimal | 'X' X</p> <p>Upper and lower bounds on points.</p> <p>Cannot be set simultaneously with <code>points</code>.</p>"},{"location":"config-v3-documentation/#solution_correct","title":"[solution_correct]","text":"<p>A section for a specific solution. This one is named <code>correct</code>.</p> <p>Example</p> <p>For a task with samples, two other tests and solutions <code>correct.py</code>, <code>wrong.cpp</code> and <code>slow.java</code> the config might look like this: <pre><code>[solution_correct]\n# primary solution must pass on all tests\nprimary=yes\n\n[solution_wrong]\n# we don't really care about samples, otherwise wrong\ntests=XWW\n\n[solution_slow]\n# timeout on test02, otherwise correct\ntests=11T\n</code></pre></p>"},{"location":"config-v3-documentation/#run","title":"[run]","text":"<p>Sections describing how to run a program.</p> <p>The run section for each program is optional. If it is missing, its contents are autogenerated, by looking for most specific section with the given key set, according to this hierarchy: <pre><code>graph TD;\n    RG[\"[run_gen]\"] --&gt; R[\"[run]\"];\n    RV[\"[run_validator]\"] --&gt; R;\n    RJ[\"[run_judge]\"] --&gt; R;\n    RS[\"[run_solution]\"] --&gt; R;\n    RGP[\"[run_gen:{program}]\"] --&gt; RG;\n    RVP[\"[run_validator:{program}]\"] --&gt; RV;\n    RJP[\"[run_judge:{program}]\"] --&gt; RJ;\n    RPS[\"[run_primary_solution]\"] --&gt; RS;\n    RSS[\"[run_secondary_solution]\"] --&gt; RS;\n    RSP[\"[run_solution:{program}]\"] --&gt; RPS;\n    RSP --&gt; RSS;</code></pre></p> <p>Note</p> <p>Tasks usually only have a couple of <code>[run]</code> sections: <pre><code>[run_solution]\ntime_limit=1\nsubdir=solutions\n\n[run_judge]\ntime_limit=2\n</code></pre></p>"},{"location":"config-v3-documentation/#run-build","title":"build","text":"<p>2.0.0 build_ref @auto</p> <p>Reference to the build section specifying how to build this program.</p> <p><code>@auto</code> expands to <code>{program_role}:{subdir}/{program}</code>, e.g. <code>solution:solutions/solve</code>.</p> <p>See <code>[build]</code> for more.</p>"},{"location":"config-v3-documentation/#exec","title":"exec","text":"<p>1.6.0 path </p> <p>Filename of the program to execute relative to the built directory. If a directory is chosen <code>{exec}/run</code> is executed instead.</p> <p>Defaults to empty (which chooses the built file / directory itself).</p>"},{"location":"config-v3-documentation/#args","title":"args","text":"<p>1.6.0 string </p> <p>Additional arguments for the program. (Given before any other arguments.)</p>"},{"location":"config-v3-documentation/#subdir","title":"subdir","text":"<p>1.6.0 path </p> <p>Subdirectory where to look for the program. (If empty, the task root is used.) Only used if <code>build</code> is not set.</p>"},{"location":"config-v3-documentation/#time_limit","title":"time_limit","text":"<p>1.6.0 decimal 360</p> <p>Execution time limit [seconds]. (<code>0</code> for unlimited.)</p>"},{"location":"config-v3-documentation/#clock_mul","title":"clock_mul","text":"<p>1.6.0 decimal 2</p> <p>Wall clock multiplier [1]. (<code>0</code> for unlimited.)</p> <p>The wall clock limit will be computed as <code>max(time_limit * clock_mul, clock_min)</code>.</p>"},{"location":"config-v3-documentation/#clock_min","title":"clock_min","text":"<p>1.6.0 decimal 1</p> <p>Wall clock minimum [seconds].</p>"},{"location":"config-v3-documentation/#mem_limit","title":"mem_limit","text":"<p>1.6.0 int 0</p> <p>Memory limit [MB]. (<code>0</code> for unlimited.)</p>"},{"location":"config-v3-documentation/#process_limit","title":"process_limit","text":"<p>1.6.0 int 1</p> <p>Maximum number of processes. (<code>0</code> for unlimited.)</p> <p>Please keep in mind that killing multiple processes upon errors is inherently unreliable.</p> <p>Danger</p> <p>At the moment limits greater than <code>1</code> are interpreted as \"unlimited\".</p>"},{"location":"config-v3-documentation/#env_key","title":"env_{KEY}","text":"<p>2.1.0 string </p> <p>Sets an environment variable when running this program. You can use <code>${...}</code> interpolation for substituting environment variables.</p> <code>env_{KEY}</code> usage <p>For example <code>env_DEBUG=true</code> sets <code>DEBUG</code> to have value <code>true</code>.</p> <p>When running pisek with <code>TASK</code> set to <code>aplusb</code>, <code>env_DATASET=../{TASK}</code> produces <code>DATASET=../aplusb</code>. (However, the program will have no access to <code>TASK</code> by default.)</p>"},{"location":"config-v3-documentation/#build","title":"[build]","text":"<p>Sections describing how to build a program.</p> <p>The build section for each program is optional. If it is missing, its contents are autogenerated, by looking for most specific section with the given key set, according to this hierarchy: <pre><code>graph TD;\n    BG[\"[build_gen]\"] --&gt; B[\"[build]\"];\n    BV[\"[build_validator]\"] --&gt; B;\n    BJ[\"[build_judge]\"] --&gt; B;\n    BS[\"[build_solution]\"] --&gt; B;\n    BP[\"[build:{program}]\"] --&gt; B;\n    BGP[\"[build_gen:{program}]\"] --&gt; BG;\n    BVP[\"[build_validator:{program}]\"] --&gt; BV;\n    BJP[\"[build_judge:{program}]\"] --&gt; BJ;\n    BSP[\"[build_solution:{program}]\"] --&gt; BS;</code></pre> Build sections must have different <code>{program}</code> suffixes. For multi-role programs, using <code>[build:{program}]</code> is recommended.</p> <p>Note</p> <p>For single file programs in supported languages, writing your own build section is rarely needed.</p>"},{"location":"config-v3-documentation/#sources","title":"sources","text":"<p>2.0.0 list[path] @auto</p> <p>Non-empty list of sources needed for building the executable.</p> <p><code>@auto</code> expands to <code>{program}</code>.</p>"},{"location":"config-v3-documentation/#comp_args","title":"comp_args","text":"<p>2.0.0 string </p> <p>Additional compiler arguments (given after any other arguments).</p>"},{"location":"config-v3-documentation/#extras","title":"extras","text":"<p>2.0.0 list[path] </p> <p>Additional files to be copied to the build directory (with suffixes).</p>"},{"location":"config-v3-documentation/#entrypoint","title":"entrypoint","text":"<p>2.0.0 path </p> <p>For building some executables (e.g. python), an entrypoint is needed (just the filename).</p>"},{"location":"config-v3-documentation/#strategy","title":"strategy","text":"<p>2.0.0 enum auto</p> <p>Build strategy for building this program.</p> <p><code>auto</code> detects automatically which strategy to use depending on file suffixes.</p> <p>Other strategies are:</p> <ul> <li><code>python</code></li> <li><code>shell</code></li> <li><code>c</code></li> <li><code>cpp</code></li> <li><code>pascal</code></li> <li><code>java</code></li> <li><code>go</code></li> <li><code>make</code></li> <li><code>cargo</code></li> </ul>"},{"location":"config-v3-documentation/#headers_c","title":"headers_c","text":"<p>2.0.0 list[path] </p> <p>Headers to include in the <code>c</code> <code>strategy</code> (with suffixes).</p>"},{"location":"config-v3-documentation/#extra_sources_c","title":"extra_sources_c","text":"<p>2.0.0 list[path] </p> <p>Extra source files for the <code>c</code> <code>strategy</code>.</p>"},{"location":"config-v3-documentation/#headers_cpp","title":"headers_cpp","text":"<p>2.0.0 list[path] </p> <p>Headers to include in the <code>cpp</code> <code>strategy</code> (with suffixes).</p>"},{"location":"config-v3-documentation/#extra_sources_cpp","title":"extra_sources_cpp","text":"<p>2.0.0 list[path] </p> <p>Extra source files for the <code>cpp</code> <code>strategy</code>.</p>"},{"location":"config-v3-documentation/#extra_sources_java","title":"extra_sources_java","text":"<p>2.0.0 list[path] </p> <p>Extra source files for the <code>java</code> <code>strategy</code>.</p>"},{"location":"config-v3-documentation/#extra_sources_py","title":"extra_sources_py","text":"<p>2.1.0 list[path] </p> <p>Extra source files for the <code>python</code> <code>strategy</code>.</p> <p>It is recommended to set headers and extra_sources keys in higher sections (<code>[build_solution]</code>, <code>[build]</code>,...) and use the <code>sources</code> and <code>extras</code> keys for program-specific sections.</p> Stubs and headers <pre><code>[build_solution]\nheaders_c=src/guess.h\nextra_sources_c=src/stub.c\nheaders_cpp=src/guess.h\nextra_sources_cpp=src/stub.cpp\nextra_sources_py=src/python.py\n</code></pre>"},{"location":"config-v3-documentation/#limits","title":"[limits]","text":"<p>Section with input/output size limits.</p>"},{"location":"config-v3-documentation/#input_max_size","title":"input_max_size","text":"<p>1.0.0 int 50</p> <p>Maximum input size [MB]. (<code>0</code> for unlimited.)</p>"},{"location":"config-v3-documentation/#output_max_size","title":"output_max_size","text":"<p>1.0.0 int 10</p> <p>Maximum output size [MB]. (<code>0</code> for unlimited.)</p>"},{"location":"config-v3-documentation/#cms","title":"[cms]","text":"<p>Settings related to the CMS importer. See CMS docs for details.</p>"},{"location":"config-v3-documentation/#name_1","title":"name","text":"<p>1.6.0 string </p> <p>The name of the task, which will appear in the task URL (required for CMS commands).</p>"},{"location":"config-v3-documentation/#title","title":"title","text":"<p>1.0.0 string @name</p> <p>The name of the task shown on the task description page.</p> <p><code>@name</code> expands to the task name.</p>"},{"location":"config-v3-documentation/#submission_format","title":"submission_format","text":"<p>1.0.0 string @name</p> <p>The name of the submitted file. <code>.%l</code> will be replaced with the language's file extension.</p> <p><code>@name</code> expands to to the task name with non-alphanumeric characters replaced with <code>_</code> and <code>.%l</code> appended.</p>"},{"location":"config-v3-documentation/#time_limit_1","title":"time_limit","text":"<p>1.0.0 decimal 1</p> <p>Execution time limit [seconds].</p>"},{"location":"config-v3-documentation/#mem_limit_1","title":"mem_limit","text":"<p>1.0.0 int 1024</p> <p>Memory limit [MB].</p>"},{"location":"config-v3-documentation/#max_submissions","title":"max_submissions","text":"<p>1.0.0 int | 'X' 50</p> <p>The number of submissions one contestant is allowed to make, or <code>X</code> for unlimited.</p>"},{"location":"config-v3-documentation/#min_submission_interval","title":"min_submission_interval","text":"<p>1.0.0 int 0</p> <p>The number of seconds a contestant has to wait between consecutive submissions.</p>"},{"location":"config-v3-documentation/#score_mode","title":"score_mode","text":"<p>1.0.0 enum max_subtask</p> <p>Describes how the final score is computed from the scores of individual submissions. May be <code>max</code>, <code>max_subtask</code> or <code>max_tokened_last</code>.</p>"},{"location":"config-v3-documentation/#feedback_level","title":"feedback_level","text":"<p>1.0.0 enum restricted</p> <p>Specifies how much information is given to the contestants about their submission. May be <code>full</code>, <code>restricted</code> or <code>oi_restricted</code>.</p>"},{"location":"config-v3-documentation/#stubs","title":"stubs","text":"<p>2.0.0 list[path] @auto</p> <p>Only for C/C++/Python.</p> <p>Stubs to upload to CMS (without suffix).</p> <p><code>@auto</code> expands to the union of <code>[build_solution]</code>'s <code>extra_sources_*</code> keys.</p> <p>Used commonly for interactive tasks.</p>"},{"location":"config-v3-documentation/#headers","title":"headers","text":"<p>2.0.0 list[path] @auto</p> <p>Only for C/C++/Python.</p> <p>Stubs to upload to CMS (without suffix).</p> <p><code>@auto</code> expands to the union of <code>[build_solution]</code>'s <code>headers_*</code> keys.</p> <p>Used commonly for interactive tasks.</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install Pisek simply by running: <pre><code>pip install pisek\n</code></pre></p> <p>For upgrading, add <code>--upgrade</code>: <pre><code>pip install pisek --upgrade\n</code></pre></p> <p>Or if you can't wait for the latest features, you can download pisek directly from GitHub : <pre><code>pip install git+https://github.com/piskoviste/pisek\n</code></pre></p>"},{"location":"quickstart/","title":"Usage","text":""},{"location":"quickstart/#creating-a-task","title":"Creating a task","text":"<p>First, you will need a task to test. You can create a task skeleton by running:</p> <pre><code>pisek init\n</code></pre> <p>You can also choose one of our example tasks to try pisek on.</p>"},{"location":"quickstart/#what-a-task-looks-like","title":"What a task looks like","text":"<p>A task is a single directory containing programs, task data, and most importantly a <code>config</code> file which holds all the metadata for the task (scoring, limits, how to run programs, etc.).</p> <p>Programs have their roles (generator, solution, judge, \u2026) specified in the <code>config</code>. However, it is customary to also give them self-descriptive names such as <code>gen.py</code> or <code>solve_slow.cpp</code>.</p> <p>Additionally, static inputs (<code>*.in</code>) and outputs (<code>*.out</code>) are contained in the top-level folder or in <code>static_subdir</code>, as specified in the <code>config</code>.</p>"},{"location":"quickstart/#testing-tasks","title":"Testing tasks","text":"<p>The task is tested by running: <pre><code>pisek test\n</code></pre></p> <p>For testing only some solutions, you can use: <pre><code>pisek test solutions solution1 solution2 ...\n</code></pre></p> <p>Similarly, for testing just the generator: <pre><code>pisek test generator\n</code></pre></p>"},{"location":"quickstart/#cleaning","title":"Cleaning","text":"<p>During task testing, pisek generates binaries, testing data, caches, logs, etc. You can remove all these files with: <pre><code>pisek clean\n</code></pre></p>"},{"location":"quickstart/#visualization","title":"Visualization","text":"<p>Finally, you can visualize solution times and their closeness to the time limit: <pre><code>pisek test --testing-log\npisek visualize\n</code></pre></p>"},{"location":"task-overview/","title":"Task overview","text":"<p>The Task overview is meant for those who are familiar with competitive programming but haven't made any tasks yet. If you have already set tasks before, you can just look at the task type diagrams for a batch task and an interactive task to brush up on potential differences in terminology.</p>"},{"location":"task-overview/#task-parts","title":"Task parts","text":"<p>There are several task parts the author needs to write:</p>"},{"location":"task-overview/#task-statement","title":"Task statement","text":"<p>The task statement is there for contestant to read. Pisek doesn't handle task statements, so you are free to write them however you want.</p>"},{"location":"task-overview/#tests","title":"Tests","text":"<p>Pisek however does need to know about tests (groups of testcases). Those usually are some additional restrictions on the task statements to allow weaker solutions to get some points. These can be lower limits for slower solutions, or special cases of the original task statement.</p> <p>Some tasks have only one test (i.e. ICPC tasks) while others have many (IOI tasks). Tests need to be entered in the config.</p>"},{"location":"task-overview/#solution","title":"Solution","text":"<p>The best known task component is the solution. It is the same as what the contestant should write - taking the input and producing the output.</p> <p>One of the solutions should be the primary solution, always producing the correct output. It is also recommended to write some wrong solutions to ensure they don't pass.</p>"},{"location":"task-overview/#generator","title":"Generator","text":"<p>The generator is used for generating inputs that the solution is tested on. Ideally, the generator should generate diverse enough inputs to break any wrong solution.</p>"},{"location":"task-overview/#checker","title":"Checker","text":"<p>The checker is used for determining whether a given solution is correct. It greatly differs between task types, so you can read more there.</p> <p>A task-specific checker provided by the task author is called a judge.</p>"},{"location":"task-overview/#validator","title":"Validator","text":"<p>The validator is used for making sure that inputs produced by the generator conform to the task statement. It adds an additional degree of safety.</p>"},{"location":"task-overview/#config","title":"Config","text":"<p>Bringing all of these together is the task configuration file. It contains all the metadata about the task: How programs are built, how programs are run (including limits), information about the tests and more.</p>"},{"location":"task-overview/#task-types","title":"Task types","text":"<p>There are a few types of tasks pisek supports:</p> <ol> <li>Batch tasks</li> <li>Interactive tasks</li> </ol>"},{"location":"task-overview/#batch-task","title":"Batch task","text":"<pre><code>graph TD;\n    G[Generator] --&gt;|Input| V[Validator];\n    G --&gt;|Input| S[Solution];\n    G --&gt;|Input| CS[Correct solution];\n    G --&gt;|Input| C[Checker];\n    S --&gt;|Output| C;\n    CS --&gt;|Correct output| C;</code></pre>"},{"location":"task-overview/#batch-checker","title":"Batch checker","text":"<p>A batch checker gets the solution output and determines whether it is correct. It can also get the correct output (from the primary solution) if specified in the config.</p>"},{"location":"task-overview/#interactive-task","title":"Interactive task","text":"<pre><code>graph TD;\n    G[Generator] --&gt;|Input| C[Validator];\n    G --&gt;|Input| J[Judge];\n    J ==&gt;|Communication| S[Solution];\n    S ==&gt;|Communication| J;</code></pre> <p>An interactive task is used when a part of the input should remain hidden from the contestant's solution. For example, when it can ask questions about the input.</p>"},{"location":"task-overview/#interactive-judge","title":"Interactive judge","text":"<p>The judge in an interactive task gets the task input and is run together with the solution. The solution can make requests to the judge about the input. Finally, the judge determines whether the solution is correct.</p> <p>One example would be that the judge gets a hidden sequence in the input. The solution then makes queries to the judge and reconstructs the hidden sequence. After giving this sequence to the judge, the solution is marked by the judge as correct on this input.</p>"},{"location":"task-parts/batch-checker/","title":"Batch checker","text":"<p>The batch checker gets the contestant's output, the input, and the correct output. It should determine whether the contestant's output is correct.</p> <p>There are various types of checkers you can use:</p> <ul> <li>tokens \u2013 fast, versatile file equality checker</li> <li>shuffle \u2013 similar to tokens, but allows permutations of tokens</li> <li>diff \u2013 file equality checker based on the <code>diff</code> command line tool (avoid this option, as it has quadratic time complexity)</li> <li>judge \u2013 custom checker</li> </ul> <p>If there is only a single correct output (e.g. the minimum of an array), <code>tokens</code> is strongly recommended. Otherwise, when there are multiple correct outputs (e.g. the shortest path in a graph), writing a judge is necessary. Set <code>out_check</code> accordingly.</p>"},{"location":"task-parts/batch-checker/#tokens-checker","title":"Tokens checker","text":"<p>A fast and versatile equality checker. Ignores whitespace, but not newlines. (Ignores newlines only at the end of a file.)</p> <p>Tokens are separated by (possibly multiple) whitespace characters. For the output to be correct, the tokens need to be same as in the correct output file.</p> <p>You can customize the tokens checker with <code>tokens_ignore_newlines</code> or <code>tokens_ignore_case</code>. For comparing floats, set <code>tokens_float_rel_error</code> and <code>tokens_float_abs_error</code>.</p>"},{"location":"task-parts/batch-checker/#shuffle-checker","title":"Shuffle checker","text":"<p>Similarly to the tokens checker, the shuffle checker compares the output with the correct output token-by-token. Allows permutations of tokens (configure with <code>shuffle_mode</code>). Use <code>shuffle_ignore_case</code> for case insensitivity.</p>"},{"location":"task-parts/batch-checker/#diff-checker","title":"Diff checker","text":"<p>An equality checker based on the <code>diff</code> tool. Runs <code>diff -Bbq</code> under the hood. Ignores whitespace and empty lines.</p> This <code>out_check</code> is not recommended <p>In some cases, <code>diff</code> has quadratic time complexity, leading to unexpectedly slow checking of outputs.</p>"},{"location":"task-parts/batch-checker/#custom-judge","title":"Custom judge","text":"<p>If there can be multiple correct solutions, it is necessary to write a custom judge. Set <code>out_judge</code> to the path to the source code of your judge, <code>judge_type</code> to the judge type (see below), and <code>judge_needs_in</code>, <code>judge_needs_out</code> to <code>0</code>/<code>1</code>, depending on whether the judge needs the input and the correct output.</p> <p>When writing a custom judge, you can choose from multiple judge types:</p> <ol> <li>cms-batch judge</li> <li>opendata-v2</li> <li>opendata-v1</li> </ol>"},{"location":"task-parts/batch-checker/#cms-batch-judge","title":"CMS-batch judge","text":"<p>The CMS batch judge format as described in the CMS documentation.</p> <p>It is run as follows (having filenames given as arguments): <pre><code>./judge &lt;input&gt; &lt;correct output&gt; &lt;contestant output&gt;\n</code></pre></p> <p>The judge should print a relative number of points (a float between 0.0 and 1.0) to its stdout as a single line. To its stderr it should write a single-line message to the contestant. Unlike what the CMS documentation specifies, the files should be single-line only. There will be a warning otherwise.</p> Example <code>cms-batch</code> judge <p>For a task of printing N positive integers that sum up to K, the judge may look like this: <pre><code>#!/usr/bin/env python3\nimport sys\nfrom typing import NoReturn\n\n\ndef award(points: float, msg: str) -&gt; NoReturn:\n    print(points)\n    print(msg, file=sys.stderr)\n    exit(0)\n\n\ndef reject() -&gt; NoReturn:\n    award(0, \"translate:wrong\")\n\n\ninput_, correct_output, contestant_output = sys.argv[1:]\n\nwith open(input_) as f:\n    n, k = map(int, f.readline().split())\n\nwith open(contestant_output) as f:\n    try:\n        numbers = list(map(int, f.readline().split()))\n    except ValueError:\n        reject()  # The contestant did not print integers\n    except EOFError:\n        reject()  # The contestant output ends\n\n    if f.read().strip():\n        reject()  # Contestant output doesn't end when it should\n\n\n# Be careful to check **ALL** constraints\nif any(map(lambda x: x &lt;= 0, numbers)):\n    reject()\n\nif sum(numbers) == k:\n    award(1.0, \"translate:success\")\nelif sum(numbers) &gt;= k / 2:\n    award(0.5, \"translate:partial\")\nelse:\n    reject()  # Not enough sand used\n</code></pre></p>"},{"location":"task-parts/batch-checker/#opendata-v2-judge","title":"Opendata-v2 judge","text":"<p>The opendata-v2 judge is run in this way: <pre><code>./judge &lt;test&gt; &lt;seed&gt; &lt; contestant-output\n</code></pre> Where <code>test</code> is the testcase's test number and <code>seed</code> the testcase's generating seed. (The arguments are the same as those given to the <code>opendata-v1</code> generator this input has (probably) been generated with.) If the input was not generated with a seed (static or unseeded), <code>seed</code> will be <code>-</code>.</p> <p>If <code>judge_needs_in</code> is set, the judge will get the input filename in the <code>TEST_INPUT</code> environment variable. Similarly, if <code>judge_needs_out</code> is set, the correct output filename will be in the <code>TEST_OUTPUT</code> environment variable.</p> <p>If the output is correct, the judge should exit with return code 42. Otherwise, the judge should exit return code 43.</p> <p>Optionally, the judge can write a one-line message for the contestant to stderr (at most 255 bytes), followed by a sequence of lines with <code>KEY=value</code> pairs. The following keys are allowed:</p> <ul> <li><code>POINTS</code> \u2013 Number of points awarded for this test case (used only if the exit code says \"OK\").</li> <li><code>LOG</code> \u2013 A message that should be logged.</li> <li><code>NOTE</code> \u2013 An internal note recorded in the database, but not visible to contestants.</li> </ul> <p>Values are again limited to 255 bytes.</p> Example <code>opendata-v2</code> judge <p>For a task of printing N positive integers that sum up to K, the judge may look like this: <pre><code>#!/usr/bin/env python3\nimport os\nimport sys\nfrom typing import NoReturn\n\nSUBTASK_POINTS = [0, 20, 30, 50]\n\n\ndef award(points: float | None, msg: str) -&gt; NoReturn:\n    assert \"\\n\" not in msg\n    print(msg, file=sys.stderr)\n    print(f\"POINTS={points}\", file=sys.stderr)\n    exit(42)\n\n\ndef reject(msg: str) -&gt; NoReturn:\n    assert \"\\n\" not in msg\n    print(msg, file=sys.stderr)\n    exit(43)\n\n\ndef main(subtask: int, seed: str) -&gt; NoReturn:\n    # Read the test input\n    with open(os.environ[\"TEST_INPUT\"]) as f:\n        n, k = map(int, f.readline().split())\n\n    # Read the contestant output\n    try:\n        numbers = list(map(int, input().split()))\n    except ValueError:\n        reject(\"The output should contain integers.\")\n    except EOFError:\n        reject(\"The output is empty.\")\n\n    try:\n        input()\n        reject(\"The output should contain only one line.\")\n    except EOFError:\n        pass\n\n    # Be careful to check **ALL** constraints\n    if any(map(lambda x: x &lt;= 0, numbers)):\n        reject(\"The output contains negative integers.\")\n\n    if sum(numbers) == k:\n        award(SUBTASK_POINTS[subtask], \"All of the sand used.\")\n    elif sum(numbers) &gt;= k / 2:\n        award(SUBTASK_POINTS[subtask] // 2, \"At least half of the sand used.\")\n    else:\n        reject(\"Not enough sand used.\")\n\n\nif __name__ == \"__main__\":\n    main(subtask=int(sys.argv[1]), seed=sys.argv[2])\n</code></pre></p>"},{"location":"task-parts/batch-checker/#opendata-v1-judge","title":"Opendata-v1 judge","text":"<p>The opendata-v1 judge is the same as opendata-v2, with the exception of using different return codes, return code 0 for a correct output and return code 1 for a wrong output.</p> This <code>judge_type</code> is not recommended <p>Return with exit code 1 is very common and is for example trigger by any exception in Python. This can lead to internal judge bugs disguising themselves as wrong answers.</p>"},{"location":"task-parts/generator/","title":"Generator","text":"<p>The generator is used for generating inputs. Solutions are then run and judged on those.</p>"},{"location":"task-parts/generator/#generator-type","title":"Generator type","text":"<p>There are currently 3 <code>gen_type</code>s available:</p> <ul> <li>pisek-v1</li> <li>cms-old</li> <li>opendata-v1</li> </ul> <p>However, we strongly recommend using the first (pisek-gen) type for better debugging and easy conversion between open data and closed data tasks.</p>"},{"location":"task-parts/generator/#terminology","title":"Terminology","text":"<p>There are two requirements for generators:</p> <ul> <li>Generators must be deterministic. \u2014 For the same arguments, they should always generate the same input(s).</li> <li>Generators must respect the seed \u2014 If a generator takes a seed as an argument, the generator should generate different inputs for different seeds. This can be disabled in the with <code>checks.generator_respects_seed</code>, but be careful.</li> </ul>"},{"location":"task-parts/generator/#pisek-v1","title":"Pisek-v1","text":""},{"location":"task-parts/generator/#listing-inputs","title":"Listing inputs","text":"<p>When run without arguments it should list all inputs it can generate in the following format: <pre><code>input_name key1=value1 key2=value2\n</code></pre> Where <code>input_name</code> is the name of the given input. The input will be generated into the file <code>[input_name]_[seed].in</code> or <code>[input_name]</code> (depending whether the input is seeded). This is followed by any number of key=value pairs separated by spaces. The following keys are supported:</p> Key Meaning Value type Default value repeat How many times should this input be generated? int 1 seeded Is this input generated with a random seed? bool true <p>If the input is not seeded, repeat must be 1.</p> <p>For example: <pre><code>01_tree\n02_random_graph repeat=10\n02_complete_graph seeded=false\n</code></pre></p> Example <code>pisek-v1</code> generator <p>For a task of printing N positive integers that sum up to K, the generator may look like this: <pre><code>#!/usr/bin/env python3\nimport itertools\nimport random\nimport sys\n\nFEATURES = [\"few\", \"small\", \"equal\", \"max\"]\n\n\ndef gen(input_name: str) -&gt; None:\n    max_n = 10 if \"few\" in input_name else 10**5\n    max_k = 10**5 if \"small\" in input_name else 10**9\n\n    if \"max\" in input_name:\n        n = max_n\n        k = max_k\n    else:\n        n = random.randint(1, max_n)\n        k = random.randint(n, max_k)\n\n    if \"equal\" in input_name:\n        k = n\n\n    print(n, k)\n\n\n# No arguments - List all inputs we can generate\nif len(sys.argv) == 1:\n    for variant in itertools.product([True, False], repeat=4):\n        features = [f for i, f in enumerate(FEATURES) if variant[i]]\n        if \"equal\" in features and \"small\" not in features:\n            continue  # Any equal input is small\n\n        if features:\n            print(\"_\".join(features), end=\" \")\n            if \"max\" in features:\n                # max is deterministic - no need for seed\n                print(\"seeded=false\")\n            else:\n                # Otherwise generate this input three times\n                print(\"repeat=3\")\n        else:\n            print(\"big repeat=5\")\n\n# Generate an input\nelse:\n    # Input is seeded\n    if len(sys.argv) == 3:\n        random.seed(sys.argv[2])\n\n    gen(sys.argv[1])\n</code></pre></p>"},{"location":"task-parts/generator/#generating-inputs","title":"Generating inputs","text":"<p>The generator is then repeatedly asked to generate the input <code>input_name</code> from the inputs list.</p> <p>If <code>input_name</code> is seeded, the generator is run with: <pre><code>./gen &lt;input_name&gt; &lt;seed&gt;\n</code></pre> Where <code>seed</code> is a 16-digit hexadecimal number. The generator must be deterministic and respect the seed.</p> <p>If <code>input_name</code> is unseeded, the generator is called with <pre><code>./gen &lt;input_name&gt;\n</code></pre> The generator must be deterministic.</p> <p>In either case, the generator should print the input to its stdout.</p>"},{"location":"task-parts/generator/#cms-old","title":"Cms-old","text":"<p>The generator is run with: <pre><code>./gen &lt;directory&gt;\n</code></pre></p> <p>The generator should generate all input files to this directory. The generator must be deterministic.</p> This <code>gen_type</code> is not recommended <p>We don't recommend using this generator type for these reasons:</p> <ul> <li>It's hard to debug, as you don't know during the generation of what input the bug occurs.</li> <li>You need to write your own management code for which inputs to generate, how many times, and what seeds to use.</li> </ul>"},{"location":"task-parts/generator/#opendata-v1","title":"Opendata-v1","text":"<p>The generator is run: <pre><code>./gen &lt;test&gt; &lt;seed&gt;\n</code></pre> Where <code>seed</code> is 16-digit hexadecimal number.</p> <p>The generator should generate the input for this test to its stdout. The generator must be deterministic and respect the given seed.</p> <p>(Please note that the generator is run only for tests without static outputs and may generate only one input for each test.)</p> Example <code>opendata-v1</code> generator <p>For a task of printing N positive integers that sum up to K, the generator may look like this: <pre><code>#!/usr/bin/env python3\nimport itertools\nimport random\nimport sys\n\n\ndef gen(test: int) -&gt; None:\n    max_n = 10**5\n    max_k = 10**5 if test == 2 else 10**9\n\n    # Dividing max_n by four actually makes stronger tests\n    # as sandcastles of size 1 doesn't use half of the sand\n    n = random.randint(1, max_n // 4)\n    k = random.randint(n, max_k)\n\n    if test == 1:\n        k = n\n\n    print(n, k)\n\n\nrandom.seed(sys.argv[2])\ngen(int(sys.argv[1]))\n</code></pre></p>"},{"location":"task-parts/interactive-judge/","title":"Interactive judge","text":"<p>The interactive judge gets the input and the means of communication with the contestant's solution. It should report whether the contestant's solution is correct.</p> <p>There is currently only one interactive <code>judge_type</code> supported:</p>"},{"location":"task-parts/interactive-judge/#cms-communication-judge","title":"CMS-communication judge","text":"<p>See CMS documentation for more. However, only one user process is supported.</p> <p>The solution and the judge run simultaneously. They communicate via FIFO (named pipes).</p> <p>The judge is run: <pre><code>./judge &lt;FIFO from solution&gt; &lt;FIFO to solution&gt; &lt; input\n</code></pre> FIFOs point to the solution's stdout and stdin respectively.</p> <p>The judge should print a relative number of points (a float between 0.0 and 1.0) to its stdout as a single line. To its stderr it should write a single-line message to the contestant. Unlike what the CMS documentation specifies, the files should be single-line only. There will be a warning otherwise.</p>"},{"location":"task-parts/tests/","title":"Tests","text":"<p>The solutions are judged on tests (groups of testcases) each usually representing additional restriction(s) on the task statement. Each test is worth a certain number of points, which are awarded for solving all testcases in the test.</p>"},{"location":"task-parts/tests/#testcases","title":"Testcases","text":"<p>The testcases can be of three kinds:</p> <ul> <li>static \u2014 Both the input and the output is created as a file by the task author.</li> <li>mixed \u2014 The input is written by the task author, and the correct output is generated by the primary solution.</li> <li>generated \u2014 The input is generated by the generator, the correct output by the primary solution.</li> </ul> <p>For interactive tasks, we only have static and generated testcases, as there are no outputs.</p> <p>Pisek distinguishes the kind of a testcase automatically. The static and mixed testcases have an input (<code>.in</code> file) in the <code>static_subdir</code> directory. If a <code>.out</code> file with matching name is also there, the testcase is static. Otherwise, it's mixed. The list of the generated inputs is given by the generator (either by the generator itself or its interface).</p>"},{"location":"task-parts/validator/","title":"Validator","text":"<p>The validator is used for validating inputs, i.e. making sure they conform to the restrictions in the task statement.</p>"},{"location":"task-parts/validator/#validator-type","title":"Validator type","text":"<p>There are currently 2 <code>validator_type</code>s available:</p> <ul> <li>simple-42</li> <li>simple-0</li> </ul>"},{"location":"task-parts/validator/#simple-42","title":"Simple-42","text":"<p>The simple-42 validator is run as follows: <pre><code>./validate &lt;test_num&gt; &lt; input\n</code></pre> Where <code>test_num</code> is the number of the test to which the input belongs. If the input is valid, the validator should exit with return code 42. Otherwise, it should exit with any other return code.</p> Example <code>simple-42</code> validator <p>For a task of printing N positive integers that sum up to K, the validator may look like this: <pre><code>#!/usr/bin/env python3\nimport sys\n\ntest = int(sys.argv[1])\nn, k = map(int, input().split(\" \"))  # We use explicitly split(\" \") to be more strict\n\nassert 1 &lt;= n &lt;= 10**5, f\"{n=} limits\"\nassert 1 &lt;= k &lt;= 10**9, f\"{k=} limits\"\n\nif test == 1:\n    assert n == k, \"N and K should be equal\"\nelif test == 2:\n    assert k &lt;= 10**5, f\"{k=} is too big\"\n\ntry:\n    input()\nexcept EOFError:\n    exit(42)  # The input is valid\n\nassert False, \"The input doesn't end when it should\"\n</code></pre></p>"},{"location":"task-parts/validator/#simple-0","title":"Simple-0","text":"<p>The simple-0 validator is run as follows: <pre><code>./validate &lt;test_num&gt; &lt; input\n</code></pre> Where <code>test_num</code> is the number of the test to which the input belongs. If the input is valid, the validator should exit with return code 0. Otherwise, it should exit with any other return code.</p> This <code>validator_type</code> is not recommended <p>Return with exit code 0 can be sometimes caused by libraries. It is better to avoid this pitfall and use the simple-42 validator type instead.</p>"}]}